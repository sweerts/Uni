\documentclass[12pt,oneside,a4paper]{article}

\usepackage[german]{babel}
\usepackage[latin1]{inputenc}
\usepackage[draft]{graphicx}

%\usepackage{fouriernc}
 
%\usepackage{showkeys}
 
\renewcommand{\baselinestretch}{1.1}



\hfuzz 2pt

\usepackage{amsmath,amsfonts}
\usepackage{amsthm}

%\usepackage[mathcal]{euler}


\usepackage{url}


\newcommand{\dint}{\displaystyle\int}


\newcommand{\la}{\langle}


\newcommand{\ra}{\rangle}


\renewcommand{\Tilde}{\widetilde}
\renewcommand{\Hat}{\widehat}
\renewcommand{\Bar}{\overline}

\newcommand{\loc}{\mathrm{loc}}

\newcommand{\TT}{\mathbb{T}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\NN}{\mathbb{N}}

%\newcommand{\la}{\langle}
%\newcommand{\ra}{\rangle}

\newcommand{\cO}{\mathcal{O}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cF}{\mathcal{F}}

\newcommand{\ELL}{\mathcal{L}}

\DeclareMathOperator{\grad}{Grad}
\DeclareMathOperator{\glim}{glm-lim}

\renewcommand{\Re}{\mathop{\mathrm{Re}}}
\renewcommand{\Im}{\mathop{\mathrm{Im}}}


%\theoremstyle{theorem}

%\renewcommand{\proofname}{\bf Beweis}

\newtheoremstyle{mytheorem}
  {\topsep}   % ABOVESPACE
  {\topsep}   % BELOWSPACE
  {\itshape}  % BODYFONT
  {}       % INDENT (empty value is the same as 0pt)
  {\bfseries} % HEADFONT
  {.}         % HEADPUNCT
  {5pt plus 1pt minus 1pt} % HEADSPACE
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}          % CUSTOM-HEAD-SPEC

%\theoremstyle{plain}

\theoremstyle{mytheorem}

\newtheorem{satz}{Satz}
\newtheorem*{satz*}{Satz}
\newtheorem{prop}[satz]{Proposition}
\newtheorem{lemma}[satz]{Lemma}
\newtheorem{korol}[satz]{Korollar}

%\theoremstyle{remark}


\newtheoremstyle{mydefinition}
  {\topsep}   % ABOVESPACE
  {\topsep}   % BELOWSPACE
  {\normalfont}  % BODYFONT
  {}       % INDENT (empty value is the same as 0pt)
  {\bfseries} % HEADFONT
  {.}         % HEADPUNCT
  {5pt plus 1pt minus 1pt} % HEADSPACE
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}          % CUSTOM-HEAD-SPEC



\theoremstyle{mydefinition}

\newtheorem{defin}[satz]{Definition}
\newtheorem{bsp}[satz]{Beispiel}
\newtheorem{bmk}[satz]{Bemerkung}
\newtheorem{ubg}{Übung}

%\numberwithin{theorem}{sectio}
%\numberwithin{exer}{section}
%\numberwithin{equation}{section}

\newcommand{\slim}{\mathop{\mathrm{s-lim}}}

\newcommand{\eps}{\varepsilon}

\DeclareMathOperator{\ess}{ess}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\spec}{spec}
%\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\res}{res}
\DeclareMathOperator{\ran}{ran}
\DeclareMathOperator{\Ran}{ran}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\gr}{gr}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\area}{area}

\newcommand{\specd}{\spec_\mathrm{disc}}
\newcommand{\spece}{\spec_\mathrm{ess}}
\newcommand{\specp}{\spec_\mathrm{p}}

%\renewcommand{\proofname}{{\bf Proof}}
%\renewcommand{\proofname}{$\mathbf{Beweisaaa}$}
%\def\proof{\normalshape{\setlength{\parskip}{0pt}\par\addvspace\medskipamount \noindent{\bf Beweis: }}\ignorespaces}

%\parindent 0pt
%\parskip 3pt
%
\sloppy

\textwidth 150mm
\textheight 230mm
\oddsidemargin 4.6mm
\topmargin -5.4mm

\begin{document}


\pagestyle{empty}

\begin{center}
\large

~

\vfil


{\bf Analysis IIa}

\bigskip

Dozent: Konstantin Pankrashkin


\bigskip


\vfil

\end{center}

\clearpage

%{

%\footnotesize

\tableofcontents

%}

%\clearpage

\newpage

\setcounter{page}{1}

\pagestyle{plain}


\begin{center}
\large\bf Vorlesung 1
\bigskip
\end{center}

\begin{defin}
Seien $I\subset \RR$ ein Intervall und $f:I\to\RR$ eine Funktion. Eine {\bf Stammfunktion} von $f$
ist eine differenzierbare Funktion $\Phi:I\to\RR$ mit $\Phi'(x)=f(x)$ für alle $x\in I$.
\end{defin}


Wir machen ein paar einfache aber wichtige Bemerkungen.

\begin{bmk}
Manchmal ist das Intervall $I$ wichtig, in diesem Fall ist von \emph{einer Stammfunktion auf $I$} die Rede.
Ist $\Phi$ eine Stammfunktion von $f$ auf einem Intervall $I$, so ist $\Phi$ auch eine Stammfunktion von $f$
auf jedem Teilintervall von $I$.
\end{bmk}

\begin{bmk}
Ist $\Phi$ eine Stammfunktion von $f$, so hat jede Stammfunktion $\Psi$ von $f$
die Form $\Psi=\Phi+C$, wobei $C$ eine Konstante ist. (Beweis: $(\Psi-\Phi)'=\Psi'-\Phi'=f-f=0$,
also ist $\Psi-\Phi$ konstant.) Umgekehrt, jede Funktion $\Psi$ der Form $\Psi=\Phi+C$ mit einer Konstante $C$
ist eine Stammfunktion von $f$, da $(\Phi+C)'=\Phi'=f$. Die ganze Familie der Stammfunktionen von $f$ (falls sie existieren) bezeichnet man als
\[
\int f
\] 
und das Zeichen $\int$ nennt sich {\bf unbestimmtes Integral}.\footnote{Später werden Sie auch andere Integrale lernen, z.B.
$\displaystyle \int_a^b, \displaystyle\oint, \displaystyle\iint, \dint^\oplus$ usw.
} Ist $\Phi$ eine konkrete Stammfunktion von $f$, so schreibt man auch
\[
\int f(x) dx =\Phi(x)+C
\]
(damit die Variable $x$ sichtbar bleibt). 
Die Aufgabe, das unbestimmte Integral von $f$ auszurechnen, ist also äquivalent zu der Aufgabe, eine Stammfunktion von $f$ zu finden.
Insbesondere haben wir für jede differenzierbare Funktion $f$ die Gleichheit
\[
\int f'(x) dx=f(x)+C.
\]
Die Schreibweise mit $dx$ ist besonders nützlich, falls es irgenwelche Parameter im Spiel sind. Zum Beispiel,
\begin{align*}
\int (x+2t)\, \underbrace{dx}_{!}&=\dfrac{x^2}{2}+2tx+C, &&\text{ weil } \dfrac{d}{dx}\Big(\dfrac{x^2}{2}+2tx+C\Big)=x+2t,\\
\text{ aber } \int (x+2t)\underbrace{dt}_{!} &=tx+t^2+C, && \text{ weil } \dfrac{d}{dt}\big(tx+t^2+C\big)=x+2t.
\end{align*}
Die Operation $\int$ ist natürlich linear:
\[
\int (f+g) =\int f + \int g,
\quad
\int a f = a\int f
\]
(hier sind $f$ und $g$ Funktionen und $a$ eine Konstante. Beweis: $(\int f + \int g)'=(\int f)'+(\int g)'=f+g$ usw.).
\end{bmk}


\begin{bmk}
Die Existenz einer Stammfunktion ist nicht automatisch. Das sieht man mit einem ganz einfachen Beispiel:
\emph{Die Funktion $f:x\mapsto \begin{cases}
1, & \text{ für } x\ge 0,\\
-1, & \text{ für } x<0,
\end{cases}
$
besitzt keine Stammfunktion auf $\RR$.}

\begin{proof} (Widerspruchsbeweis) Sei $\Phi$ eine Stammfunktion von $f$ auf $\RR$, dann ist $\Phi$ auch eine Stammfunktion von $f$
auf $(-\infty,0)$ und auf $(0,+\infty)$. F\"ur $x<0$ hat man also $\Phi'(x)=-1$. Die Stammfunktionen von $x\mapsto -1$
haben die Form $x\mapsto -x+a$, wobei $a$ eine Konstante ist. Also gibt es eine Konstante $a$ mit $\Phi(x)=-x+a$
für alle $x<0$. Mit derselben Logik zeigt man, dass es eine Konstante $b$ gibt mit $\Phi(x)=x+b$ für alle $x>0$.
Da $\Phi$ eine Stammfunktion von $f$ ist, ist sie differenziebar und damit auch stetig. Insbesondere hat man
\[
\Phi(0)=\underbrace{\lim_{x\to 0^-} \Phi(x)}_{=a}=\underbrace{\lim_{x\to 0^+}\Phi(x)}_{=b}.
\]
Also gilt $b=a$ und $\Phi(0)=a$. Wir haben  beweisen, dass für eine Konstante $a$ gilt
\[
\Phi(x)=\begin{cases}
-x +a, & x<0,\\
a, & x=0,\\
x+a, & x>0,
\end{cases}
\]
was sich auch als $\Phi(x)=|x|+a$ für alle $x\in\RR$ schreiben lässt. Aber die Funktion $x\mapsto |x|$ ist nicht differenizerbar für $x=0$ (Analysis 1),
also kann auch $\Phi$ nicht differenzierbar sein, was mit der Definition einer Stammfunktion nicht vereinbar ist.
\end{proof}
Wir werden in nächsten Vorlesungen sehen, dass stetige Funktionen immer Stammfunktionen besitzen.
\end{bmk}

\begin{bmk}
Beim Berechnen der Ableitungen  sieht man schnell folgendes: ist $f$ eine elementare Funktion (d.h. entsteht durch endlich viele Verkettungen und algebraische Operationen mit $\exp$, $\log$, $\sin$, $\cos$ usw.), so ist auch $f'$ eine elementare Funktion. Im Allgemeinen, sind Stammfunktionen
von elementaren Funktion aber keine elementare Funktionen (Z.B. sind Stammfunktionen von $x\mapsto e^{-x^2}$ und $x\mapsto \frac{\sin x}{x}$ keine elementare Funktionen: das werden wir aber nicht beweisen.)
Natürlich ist es interessant, für möglichst viele Funktionen ihre Stammfunktionen explizit ausrechnen zu können.
Dafür kann man z.B. die aus der Analysis~1 bekannte Tabelle der Ableitungen ``umkehren'' anfangen, siehe Tabelle unten.
Auch folgende Regeln können nützlich sein:
\begin{prop}
Sei $\Phi$ eine Stammfunktion von $f$, dann
\begin{align*}
\int f(x+a)dx&=\Phi(x+a)+C, \quad a\in \RR,\\
\int f(ax) dx&=\dfrac{1}{a}\, \Phi(ax), \quad a\in \RR\setminus\{0\}.
\end{align*}
\end{prop}

\begin{proof}
Die erste Identität folgt aus $\dfrac{d}{dx} \big( \Phi(x+a)\big)= \Phi'(x+a)=f(x+a)$,
und die zweite folgt aus $\dfrac{d}{dx} \big( \Phi(ax)\big)=a \Phi'(ax)=a f(ax)$.
\end{proof}




\begin{figure}\label{}
\centering
\fbox{\begin{tabular}{cc}
$f(x)$ & $\displaystyle\int f(x) dx$ (ohne Konstante $C$)\\ \hline
$x^a$, $a\ne -1$ & $\dfrac{x^{a+1}}{a+1}$\\
$\dfrac{1}{x}$ & $\ln |x|$\\
$\sin x$ & $-\cos x$\\
$\cos x$ & $\sin x$\\
$e^{ax}$, $a\ne 0$ & $\dfrac{1}{a}\, e^{ax}$\\
$\dfrac{1}{1+x^2}$ & $\arctan x$\\
$\dfrac{1}{\sqrt{1-x^2}}$ & $\arcsin x$
\end{tabular}}
\end{figure}

\end{bmk}

Es gibt leider keine allgemeine Methode, Stammfunktionen zum Produkt $fg$ und zur Verkettung $f\circ g$
zu finden, auch wenn man Stammfunktionen von $f$ und $g$ kennt. Daher ist das Integrieren \emph{viel}
komplexer als das Differenizeren. Wir werden aber zwei wichtige Methoden
beschreiben, mit deren Hilfe man doch viele Stammfunktionen finden kann.

\begin{satz}[Partielle Integration]
Für differenzierbare Funktionen $f$ und $g$ gilt
\[
\int f' g=fg- \int f g'.
\]
\end{satz}
\begin{proof}
Wir müssen einfach zeigen, dass die Ableitung der Funktion auf der linken Seite mit der Ableitung der Funktion auf der rechten Seite übereinstimmt.
Auf der linken Seite haben wir: $(\int f' g)'=f'g$. Auf der rechten Seite nutzt man die Leibniz-Regel:
\[
\Big(fg- \int f g'\Big)'= (fg)'- fg'= f'g+fg'-fg'=f'g. \qedhere
\]
\end{proof}

Die partielle Integration macht natürlich nur dann Sinn, wenn die neue Funktion $fg'$ ``einfacher'' ist als $f'g$.

\begin{bsp}
$\dint x e^x dx$.

Hier steht unter dem Integral schon ein Produkt. Die beiden Funktionen $x$ und $e^x$
lassen sich einfach differenzieren und integrieren. Aber die Ableitung $1$ von $x$ ist ``einfacher'' als ihre Stammfunktionen $x^2/2+C$.
Est ist also günstig, $f(x)=e^x$ und $g(x)=x$ zu nehmen. Mit Hilfe der partiellen Integration bekommt man also:
\begin{align*}
\int \underbrace{x}_{g(x)} \underbrace{e^x}_{f'(x)} dx&=\underbrace{x}_{g(x)} \underbrace{e^x}_{f(x)}-\int \underbrace{1}_{g'(x)} \underbrace{e^x}_{f(x)}dx\\
&= xe^x - \int e^x dx = xe^x-e^x+C.
\end{align*}
Wichtig: es ist relativ einfach zu prüfen, ob das Ergebnis richtig ist: man muss einfach das Ergebnis differenzieren, und die Ableitung muss mit der Funktion
im Integral übereinstimmen. Hier hat man also $(x e^x - e^x +C)=(xe^x)'-(e^x)'=e^x+xe^x-e^x=xe^x$: alles ist in Ordnung.
\end{bsp}


\begin{bsp}
Manchmal steht im Integral kein explizites Produkt. Man kann aber immer mit dem Faktor $1$ multiplizieren, dabei gilt
natürlich $1=f'(x)$ für $f(x)=x$. Zum Beispiel:
\begin{align*}
\int \ln x \,dx &= \int 1\cdot \ln x \,dx= \int \underbrace{1}_{f'(x)} \underbrace{\ln x}_{g(x)}\, dx\\
&= \underbrace{x}_{f(x)} \underbrace{\ln x}_{g(x)}- \int \underbrace{x}_{f(x)}\cdot \underbrace{\dfrac{1}{x}}_{g'(x)}\,dx\\
&=x\ln x - \int 1 \,dx=x\ln x - x+C.
\end{align*}
\end{bsp}

\begin{bsp}\label{bsp-in}
Zeige, dass für jedes $n\in\NN$ das Integral $I_n(x)=\displaystyle\int \dfrac{dx}{(1+x^2)^n}$ eine elementare Funktion ist\footnote{Statt $\displaystyle\int \dfrac{1}{\text{irgendwas}}\, dx$
schreibt man fast immer $\displaystyle\int \dfrac{dx}{\text{irgendwas}}$.}.

Für $n=1$ hat man eine Funktion ``aus der Tabelle'', also $I_1(x)=\arctan x+C$. Für grössere $n$ kann man versuchen, die Rechnung für $I_n$
auf die Rechnung für $I_{n-1}$ zu reduzieren. Dabei hilft die partielle Integration mit $f(x)=x$:
\begin{equation}
\label{eq001}
\begin{aligned}
I_n(x)&=\dint \underbrace{1}_{f'(x)}\cdot\underbrace{\dfrac{1}{(1+x^2)^n}}_{g(x)}\, dx\\
&=\underbrace{x}_{f(x)}\cdot\underbrace{\dfrac{1}{(1+x^2)^n}}_{g(x)}- \dint \underbrace{x}_{f(x)} \cdot \underbrace{\bigg( -\dfrac{n \cdot 2x}{(1+x^2)^{n+1}}\bigg)}_{g'(x)}\, dx\\
&=\dfrac{x}{(1+x^2)^n}+2n \dint \dfrac{x^2}{(1+x^2)^{n+1}}\, dx.
\end{aligned}
\end{equation}
Zur Zeit sieht man überhaupt nicht, warum diese Rechnung hilft: das Integral auf der rechten Seite
sieht nicht einfacher aus als das usprüngliche Integral. Aber man kann
die Identität $1-1=0$ richtig einsetzen:
\begin{align*}
\dint \dfrac{x^2}{(1+x^2)^{n+1}}\, dx&=\dint \dfrac{0+x^2}{(1+x^2)^{n+1}}\, dx=\dint \dfrac{-1+ (1+ x^2)}{(1+x^2)^{n+1}}\, dx\\
&=\dint \Big(-\dfrac{1}{(1+x^2)^{n+1}} + \dfrac{1}{(1+x^2)^n}\Big)\, dx=-I_{n+1}(x)+I_{n}(x).
\end{align*}
Mit Hilfe der letzten Identität kann man \eqref{eq001} wie folgt umschreiben:
\[
I_n(x)=\dfrac{x}{(1+x^2)^n} +2n \big( -I_{n+1}(x) + I_{n}(x)\big),
\]
also $I_{n+1}=\dfrac{1}{2n}\, \dfrac{x}{(1+x^2)^n} + \dfrac{2n-1}{2n} \, I_n(x)$.
Ist $I_n$ eine elementare Funktion, so ist auch $I_{n+1}$ eine elementare Funktion. Da sich $I_1$ explizit ausrechnen lässt,
sind alle $I_n$ elementare Funktionen.
\end{bsp}

Jetzt kommt die zweite wichtige Methode:
\begin{satz}[Substitution]
Seien $\varphi$ eine differenzierbare Funktion auf einem Intervall $I$, $J$ ein Intervall mit $\varphi(I)\subset J$,
$f:J\to \RR$ eine Funktion und $\Phi:J\to \RR$ ihre Stammfunktion. Dann gilt
\[
\int f\big(\varphi(x)\big) \varphi'(x)\, dx = \Phi\big(\varphi(x)\big) +C.
\]
\end{satz}
\begin{proof} Mit unseren Voraussetzungen ist $x\mapsto \Phi\big(\varphi(x)\big)$
differenzierbar auf $I$, und die Ableitung davon ist (Kettenregel) $\Phi'\big(\varphi(x)\big)\varphi'(x)$.
Das ist genau die Funktion unter dem Integral auf der linken Seite.
\end{proof}
Beim Rechnen nutzt man oft eine andere Schreibweise:
\[
\int f\big(\varphi(x)\big)\varphi'(x)\, dx=\int f(\varphi)\, d\varphi \Big|_{\varphi=\varphi(x)},
\]
wobei die Substitution $\varphi=\varphi(x)$ ganz am Ende gemacht wird.
Die Rechnungen werden übersichtlicher, falls man $\varphi'(x)dx$ als $d\varphi$ schreibt.
\begin{bsp} $\dint x e^{x^2}\, dx$.

1. Lösung: man sieht, dass $x$ ``fast'' die Ableitung von $x^2$ is (es fehlt nur ein konstanter Faktor).
Man führt also $\varphi(x)=x^2$ ein und schreibt das Integral um:
\begin{multline*}
\dint x e^{x^2}\, dx=\dfrac{1}{2} \dint 2x e^{x^2}\, dx= \dfrac{1}{2} \dint \varphi'(x) e^{\varphi (x)}\, dx \\
= \dfrac{1}{2} \int e^\varphi\, d\varphi=\dfrac{1}{2}\, e^\varphi +C= \dfrac{1}{2}\, e^{x^2}+C.
\end{multline*}

2. Lösung: am meisten stört uns (=den Dozenten) der Term $x^2$ in der Exponente. Wir können also $x=\sqrt{t}$ einführen um $e^t$ statt $e^{x^2}$ zu haben
(da $t=x^2$), dann  $dx =(\sqrt t)'dt= \dfrac{1}{2\sqrt t}\, dt$ und
\[
\dint x e^{x^2}\, dx= \dint \sqrt{t} e^t \dfrac{1}{2\sqrt{t}}\, dt=\dfrac{1}{2}\dint e^t+C=\dfrac{1}{2}\, e^t+ C =\dfrac{1}{2}\, e^{x^2}+C.
\]
\end{bsp}

\begin{bsp}
Die Substitution nutzt man oft beim Integrieren von trigonometrischen Funktionen. Zum Beispiel $\dint \cos^3 x\, dx$:
\begin{align*}
\dint \cos^3 x\, dx&=\dint \cos^2 x \cdot \cos x\, dx=\dint (1-\sin^2 x) \cos x\, dx.
\end{align*}
Hier kann man $\varphi(x)=\sin x$ einführen, dann $\cos x \, dx= d\varphi$ und
\[
\dint (1-\sin^2 x) \cos x\, dx=\dint (1-\varphi^2)\, d\varphi=\varphi-\dfrac{\varphi^3}{3} +C
=\sin x-\dfrac{\sin^3 x}{3}+C.
\]
\end{bsp}
Im letzten Beispiel sieht man, dass algebraische Identitäten sehr nutzlich sein können (hier $\cos^2 x+\sin^2 x=1$).
Insbesondere kann man mit algebraischen Ideen
eine grosse Klasse von Funktionen finden, deren Stammfunktionen elemntare Funktionen sind.

\begin{defin}
Eine {\bf rationale Funktion} ist eine Funktion der Form $x\mapsto \dfrac{P(x)}{Q(x)}$
mit Polynomen $P$ und $Q$ (mit reellen Koeffizienten).
\end{defin}

\begin{satz} Jede rationale Funktion besitzt eine elementare Stammfunktion.
\end{satz}

\begin{proof}
Unser Beweis wird auch eine Berechnungmethode liefern. Seien $P$ und $Q$ Polynome und $f=\dfrac{P}{Q}$. Wir wollen also $\int f$ ausrechnen.

{\bf Schritt 1.} Ist $Q$ konstant, dann ist $f$ auch ein Polynom, und seine Stammfunktionen sind Polynome (und damit Elementarfunktionen).
Man kann also annehmen, dass $Q$ nichtkonstant ist.

{\bf Schritt 2.} Wir können $P$ wie folgt darstellen: $P(x)=p_1(x)Q(x)+p_0(x)$, wobei $p_0$ und $p_1$ Polynome sind
mit $\grad p_0<\grad Q$. Dann gilt $f=p_1+\dfrac{p_0}{Q}$ und $\int f=\int p_1 + \int \dfrac{p_0}{Q}$.
Das Integral $\int p_1$ lässt sich explizit ausrechnen (das ist wieder ein Polynom), mann muss sich also nur um den zweiten Term kümmern.

Nach den Schritten 1 und 2 wird es klar, dass man vom Anfang an voraussetzen kann, dass $Q$ nichtkonstant ist und dass $\grad P<\grad Q$.
Ab jetzt werden wir nur diesen Fall betrachten.


{\bf Schritt 3.} Wir nutzen jetzt den Hauptsatz der Algebra: jedes nichtkonstantes komplexes Polynom besitzt (mindestens) eine komplexe Nullstelle.
Seit $a$ eine Nullstelle von $Q$, dann gilt $Q(x)=(x-a)Q_0(x)$, wobei $Q_0$ ein Polynom ist
mit $\grad Q_0=\grad Q-1$. Man kann auf dieselbe Weise auch $Q_0$ zerlegen, am Ende landet man bei der Darstellung
\[
Q(x)=A \prod_{j=1}^k (x-a_j)^{m_k}, \quad A\in\RR, \quad a_j\in\CC.
\]
Hier sind $a_j$ die Nullstellen von $Q$ (und wir nehmen an, dass $a_i\ne a_j$ für $i\ne j$) und $m_j$
sind ihre Multiplizitäten: man hat $m_j\ge 1$ und $m_1+\dots+m_k=\grad Q$.
In unserem Fall ist $Q$ ein Polynom mit rellen Koeffizienten, und das ist eine zusätzliche Eigenschaft, die man wie folgt nutzen kann:
falls $a$ eine nichtrelle Nullstelle von $Q$ ist, dann ist auch $\Bar a$ Nulstelle mit derselben Multiplizität. Ist $m$ die zugehörige Multiplizität,
so lässt sich das Produkt $(x-a)^m(x-\Bar a)^m$ als $(x^2+2bx +c)^m$ schreiben mir $b=-\Re a$ und $c=|a|^2$ (insbesondere gilt $c-b^2=|\Im a|^2>0$).
Durch solches Gruppieren der Faktoren kann man also $Q$ wie folgt zerlegen:
\[
Q(x)=A \prod_{j=1}^\ell (x-a_j)^{m_j} \prod_{j=1}^s (x^2+2b_j x+c_j)^{k_j}
\]
mit $a_j\in\RR$ (immer noch $a_i\ne a_j$ für $i\ne j$), $b_j,c_j\in \RR$ mit $c_j-b_j^2>0$,
und $m_j,k_j\ge 1$ mit $m_1+\dots+m_\ell+2k_1+\dots2k_s=\grad Q$.

{\bf Schritt 4.} Man kann beweisen, dass es reelle Konstanten $A_{j,k}$, $B_{j,k}$, $C_{j,k}$ existieren\footnote{Wir verzichten auf Beweis, da dieser sehr algebraisch ist und ganz wenig mit der Analysis zu tun hat.} mit
\begin{equation}
  \label{eq-pbz}
\dfrac{P(x)}{Q(x)}=\sum_{j=1}^\ell\sum_{k=1}^{m_j} \dfrac{A_{j,k}}{(x-a_j)^k}
+\sum_{j=1}^s \sum_{k=1}^{k_j} \dfrac{B_{j,k} x+ C_{j,k}}{(x^2+2b_j x+ c_j)^k}.
\end{equation}
Die Darstellung \eqref{eq-pbz} nennt man die \emph{Partialbruchzerlegung} von $f=P/Q$. Um unsere Behauptung zu beweisen,
müssen wir lediglich zeigen, dass man eine elementare Stammfunktion zu jedem Term auf der rechten Seite von \eqref{eq-pbz} finden kann.
Es gibt aber nur Terme von zwei Typen, die wir jetzt einzeln betrachten.
Für relle $a$ hat man 
\[
\int \dfrac{dx}{(x-a)^m}=\begin{cases} \ln|x-a|+C, & m=1,\\
 -\dfrac{1}{(m-1)(x-a)^{m-1}}+C, &m\ge 2,
\end{cases}
\]
damit sind alle Terme in der ersten Summe schon gedeckt. Jetzt müssen wir nur
$\dint \dfrac{B x+ C}{(x^2+2b x+ c)^m}\,dx$ mit $c-b^2>0$ ausrechnen. Wir nutzen die Substitution $x=\sqrt{c-b^2}t-b$, dann gilt $dx=\sqrt{c-b^2}$ und
\begin{multline*}
\int \dfrac{B x+ C}{(x^2+2b x+ c)^m}dx\\=B (c-b^2)^{1-m}\dint \dfrac{ t \,dt}{(1+t^2)^m} + (C-Bb) (c-b^2)^{\frac12-m} \dint \dfrac{ dt}{(1+t^2)^m}.
\end{multline*}
Das zweite Integral ist das im Beispiel~\ref{bsp-in} betrachtete $I_m$, und das ist also eine elementare Funktion.
Im ersten Integral kann man $\varphi(t)=t^2$ einführen, dann ist
\[
\dint \dfrac{ t dt}{(1+t^2)^m}=\dfrac{1}{2} \dint\dfrac{d\varphi}{(1+\varphi)^m}=\dfrac{1}{2}\,\begin{cases} \ln(1+\varphi)+C, & m=1,\\
-\dfrac{1}{(m-1)(1+\varphi)^{m-1}}+C, &m\ge 2,
\end{cases} 
\]
eine elementare Funktion.
\end{proof}


\begin{bsp}
$\dint \dfrac{dx}{x^2-1}$.

Die Nullstellen von $x^2-1$ sind $\pm1$, und $x^2-1=(x-1)(x+1)$. Es müssen also $A,B\in\RR$ existieren mit
\[
\dfrac{1}{x^2-1}=\dfrac{A}{x-1}+\dfrac{B}{x+1}.
\]
Die genauen Werte von $A$ and $B$ findet man durch Koeffizientenvergleich:
\[
\dfrac{A}{x-1}+\dfrac{B}{x+1}=\dfrac{A(x+1)+B(x-1)}{x^2-1}=\dfrac{(A+B)x+(A-B)}{x^2-1}.
\]
Für alle $x$ muss also $(A+B)x+(A-B)=1$ gelten, d.h. $A+B=0$ und $A-B=1$. Damit findet man $A=\frac12$ und $B=-\frac 12$, und
\[
\dint \dfrac{dx}{x^2-1}=\dfrac{1}{2} \dint \dfrac{dx}{x-1}-\dfrac{1}{2}\dint \dfrac{d}{x+1}
=\dfrac{1}{2}\ln|x-1|-\dfrac{1}{2}\ln|x+1|+C.
\]
\end{bsp}
Weitere (und kompliziertere) Beispiele werden in der Übung betrachtet.

\newpage

\begin{center}
\large\bf Vorlesung 2
\bigskip
\end{center}

In der ersten Vorlesung haben wir gesehen, dass man für bestimmte Funktionen $f$ ihre Stammfunktionen explizit ausrechnen kann. Uns fehlt aber die Theorie: wir wissen noch nicht, unter welchen Bedingungen eine Stammfunktion überhaupt existiert (wir haben ja gesehen, dass manche Funktionen keine Stammfuktionen besitzen).
Diese Frage wird jetzt besprochen.


Die Grundidee kann man zuerst sehr informell beschreiben. Betrachten wir eine positive und stetige Funktion $f:[a,b]\to \RR$.
Ist $\Phi$ eine Stammfunktion von $f$ auf $(a,b)$ und $x_0\in(a,b)$, so gilt
\[
\lim_{\Delta x\to 0^+} \dfrac{\Phi(x_0+\Delta x)-\Phi(x_0)}{\Delta x}=f(x_0).
\]
Dass bedeutet, dass für kleine $\Delta x$ die Differenz $\Phi(x_0+\Delta x)-\Phi(x_0)$ durch $f(x_0)\Delta x$ ``gut approximiert'' werden kann.
Die Zahl $f(x_0)\Delta x$ ist aber genau der Flächeninhalt eines Recktecks mit Seitenlängen $f(x_0)$ und $\Delta x$.
Betrachten wir also das Gebiet
\[
\Delta \Omega:=\big\{(x,y): x_0\le x \le x_0+\Delta x,\, 0\le y\le f(x)  \big\},
\quad
\parbox{50mm}{\includegraphics[width=40mm]{fig1.eps}}
\]
dann kann man in gewissem Sinne $\Delta\Omega$ durch das Rechteck $(x_0,x_0+\Delta x)\times\big(0, f(x)\big)$ approximieren,
also approximiert man auch $\Phi(x_0+\Delta x)-\Phi(x_0) \simeq |\Delta\Omega|$, wobei $|\cdot|$ für den Flächeninhalt steht,
und $\Phi(x_0+\Delta x)=\Phi(x_0)+|\Delta\Omega|$. Mann könnte also versuchen, die folgende Funktion $\Phi$ zu definieren:
\[
\begin{aligned}
\Phi(x_0)&=\text{Flächeninhalt der Figur $\Omega(x_0)$},\\
\Omega(x_0)&=\big\{(x,y): a\le x \le x_0,\, 0\le y\le f(x)  \big\},
\end{aligned}
\quad
\parbox{40mm}{\includegraphics[height=40mm]{fig2.eps}}
\]
und kann mann hoffen, dass $\Phi'(s)=f(x)$. Es entstehen aber gleich mehrere mathematische Fragen, insbesondere:
in welchem Sinne gelten die obigen Approximationen? was ist überhaupt der Flächeninhalt einer Figur und warum ist dieser wohldefiniert?

Wir werden uns zuerst mit der zweiten Frage beschäftigen (Flächeninhalt). Wir wissen nicht, wie man den Flächeninhalt definiert, können aber einige Eigeschaften
erwarten. Zum Beispiel muss für den Flächeninhalt $|A|$ einer Figur immer $|A|\ge 0$. Ist $A$ ein Rechteck mit Seitenlängen $a$ und $b$, so gilt
$|A|=ab$. Für $A\subset B$ würde man $|A|\le |B|$ erwarten, und für $A\cap B=\emptyset$ wäre $|A\cup B|=|A|+|B|$ eine vernünftige Eigenschaft.
Mit diesen Eigenschaften berechnet man Flächeninhalten von Figuren, die man als endliche Vereinigungen von disjunkten Rechtecken darstellen kann.
Dann kann man versuchen, komplexe Figuren durch endliche Familien von Rechecken zu approximieren, und dadurch landet
man bei Begriff eines bestimmen Integrals. Nach dieser informellen Diskussion kommen wir zurück in die mathematische Welt.

Seien $a,b\in\RR$ mit $a<b$.
\begin{defin}\label{def-treppe1}
Eine Funktion $f:[a,b]\to\RR$ heisst {\bf Treppenfunktion}, falls man ein $n\in\NN$
und Punkte $x_0,x_1,\dots, x_n\in[a,b]$ finden kann, die folgende Eigenschaften erfüllen:
\begin{itemize}
\item $a=x_0<x_1<\dots<x_n=b$,
\item $f$ ist konstant auf jedem der Intervalle $(x_{j-1},x_j)$,  $j=1,\dots,n$.
(Die Werte von $f$ an den Punkten $x_j$ haben keine Bedeutung und können beliebig sein.)
\end{itemize}
Die Menge aller solchen Funktionen werden wir als $T[a,b]$ bezeichnen.
\end{defin}

\begin{defin}\label{def-intrep}
Seit $f$ als in Definition~\ref{def-treppe1} und $f(x)=c_j$ für $x\in(x_{j-1},x_j)$.
Wir definieren
\[
\int_a^b f=\sum_{j=1}^n c_j (x_j-x_{j-1}).
\]
Diese Zahl nennt man {\bf das (bestimmte) Integral von $f$ von $a$ bis $b$ (oder zwischen $a$ und $b$)}. Oft nutzt man auch die detallierte Schreibweise $\displaystyle\int_a^b f(x)\, dx$. 
\end{defin}

\begin{bmk}\label{bmk19}
Falls man die oben genanten Eigenschaften der Flächeninhalte berücksichtigt, kann man die Zahl $I=\displaystyle\int_a^b f$ wie folgt interpretieren:
\begin{itemize}
\item Für $f\ge 0$ gilt $I=|\Omega|$ mit
\[
\Omega:=\{(x,y): a\le x\le b,\, 0\le y\le f(x)\}.
\parbox{40mm}{\includegraphics[width=38mm]{fig3.eps}}
\]
\item Für beliebige $f$ gilt $I=|\Omega_+|-|\Omega_-|$
mit
\[
\begin{aligned}
\Omega_+&:=\{(x,y): a\le x\le b,\, 0\le y\le f(x)\},\\
\Omega_-&:=\{(x,y): a\le x\le b,\, f(x)\le y\le 0\},
\end{aligned}
\quad
\parbox{60mm}{\includegraphics[width=60mm]{fig4.eps}}
\]
\end{itemize}
\end{bmk}



\begin{bsp}
Definiere $f:[0,1]\to\RR$ durch
\[
f(x)=\begin{cases} -1, & x\in[0,\frac{1}{3}],\\
2, & x\in(\frac{1}{3},1].\end{cases}
\]
Das ist eine Treppenfunktion: man kann z.B. $x_0=1$, $x_1=\frac{1}{3}$, $x_2=1$,
$c_1=-1$, $c_2=2$ nehmen, und
\[
\int_0^1 f= -1(\tfrac{1}{3}-0)+2(1-\tfrac{1}{3})=-\frac{1}{2}+\frac{4}{3}=1.
\]
\end{bsp}

Es gibt aber ein kleines Problem: die Definition des Integrals nutzt die Zerlegung von $[a,b]$ in Teilintervalle
$(x_{j-1},x_j)$, und diese Zerlegung ist nicht eindeutig: z.B. kann man diese Zerlegung ``verfeinern'',
indem man ein Intervall $(x_{j-1},x_j)$ ``künstlich'' in zwei Teilintervalle zerlegt: auf jedem dieser Intervalle ist die Funktion
immer noch konstant.
Deswegen muss man zuerst prüfen, ob  die oben definierte Zahl von der Wahl der $x_j$ unabhängig ist.
Das wird im folgenden Lemma geklärt:

\begin{lemma}
$\dint_a^b f$ ist unabhängig von der Wahl der $x_j$. 
\end{lemma}

\begin{proof}
Sei $f$ als in der Definition~\ref{def-treppe1}. Wir wählen ein $k\in\{1,\dots,n\}$
und ein $x'\in (x_{k-1},x_k)$, damit erhalten wir eine neue Zerlegung
$a=x_0<x_1<\dots<x_{k-1}<x'<x_k<\dots<x_n=b$. Dann gilt immer noch $f(c)=c_j$ für $x\in(x_{j-1},x_j)$ mit $j\ne k$
und $f(x)=c_k$ für $x\in(x_{k-1},x')$ und $x\in(x',x_k)$. Der zugehörige Wert des Integrals ist
\begin{align*}
\sum_{j\ne k} c_j(x_j-x_{j-1})+c_k(x'-x_{k-1})+c_k(x_k-x')&=\sum_{j\ne k} c_j(x_j-x_{j-1})+c_k(x_k-x_{k-1})\\
&=\sum_{j=1}^n c_j(x_j-x_{j-1}),
\end{align*}
d.h. man bekommt denselben Wert als für die usprüngliche Zerlegung $(x_j)$. Durch Induktion ist dann klar,
dass man auch nach dem Verfeinern durch endlich neue viele Punkte denselben Wert der Summe bekommt.

Seien jetzt $a=x_0<x_1<\dots<x_n=b$ und $a=y_0<\dots<y_m=b$ zwei beliebige Zerlegungen zu $f$.
Wir betrachten die Menge $Z=\{x_0,\dots,x_n,y_0,\dots,y_m\}$. Sei $N$ die Anzahlt der Elemente in $Z$,
die wir als $a=z_0<\dots<z_N=b$. Diese Zerlegung ist eine Verfeinerung von $(x_j)$ und $(y_j)$,
und die Summen für $(x_j)$ und $(y_j)$ sind gleich der Summe für $(z_j)$, daher sind sie gleich.
\end{proof}

\begin{lemma}\label{lem-inti}
Die Abbildung
\[
T[a,b]\ni f\mapsto \dint_a^b f\in\RR,
\]
erfüllt folgende Eigenschaften:
\begin{enumerate}
\item[(a)] für alle $\alpha\in\RR$ und $f\in T[a,b]$ gilt $\dint_a^b \alpha f= \alpha\dint_a^b f$,
\item[(b)] für alle $f,g\in T[a,b]$ gilt $f+g\in T[a,b]$ und $\dint_a^b (f+g)=\dint_a^b f+\dint_a^b g$,
\item[(c)] für alle $f\in T[a,b]$ gilt $\Big|\dint_a^b f\Big|\le \sup_{x\in[a,b]} \big|f(x)\big| (b-a)$,
\item[(d)] für alle $f\in T[a,b]$ mit $f\ge 0$ gilt $\dint_a^b f\ge 0$. Insbesondere gilt für alle $f,g\in T[a,b]$ mit $f\le g$
die Ungleichung $\dint_a^b f\le \dint_a^b g$.
\end{enumerate}
\end{lemma}

\begin{proof}
Die Eigenschaft (a) folgt direct aus der Definition. Die Eigenschaft (b) wird im Übungsblatt beweisen.
Für  (c) merken wir zuerst, dass falls $f$ auf einem Teilintevrall konstant ist, dann ist auch
$|f|$ auf diesem Teilintervall konstant. Daher $|f|\in T[a,b]$. Mit Bezeichnungen der Definition~\ref{def-intrep}
haben wir $\sup_{x\in[a,b]} \big|f(x)\big|\ge \max_j |c_j|$  (man hat nur $\ge$ und nicht $=$, weil auch die Werte $f(x_j)$
in $\sup$ berücksichtigt werden) und
\begin{align*}
\Big|\dint_a^b f\Big|=\Big|\sum_{j=1}^n c_j(x_j-x_{j-1})\Big|&\le \sum_{j=1}^n |c_j|(x_j-x_{j-1})\Big|\\
&\le 
\sup_{x\in[a,b]} \big|f(x)\big| \sum_{j=1}^n (x_j-x_{j-1})=\sup_{x\in[a,b]} \big|f(x)\big| (b-a).
\end{align*}
Für (d) sehen wir zuerst, dass für $f\ge 0$ alle Terme in der Summe für das Integral nichtnegativ sind, also $\dint_a^b f\ge 0$.
Für $f\le g$ hat man $g-f\ge 0$ und
\[
\int_a^b g-\int_a^b f=\int_a^b(g-f)\ge 0, \text{ also } \int_a^b f\le \int_a^b g. \qedhere
\]
\end{proof}

Jetzt möchten wir $\dint_a^b f$ für eine grössere Klasse von Funktionen $f$ definieren.
Zur Erinnerung, eine Folge $(f_n)$ von Funktionen $f_n:[a,b]\to\RR$ \emph{konvergiert gleichmässig}
gegen eine Funktion $f:[a,b]\to\RR$, falls
\[
\lim_{n}\sup_{x\in[a,b]}\big|f_n(x)-f(x)\big|=0,
\]
und in diesem Fall schreiben wir $f=\glim_{n\to\infty} f_n$.
\begin{defin}
Eine Funktion $f:[a,b]\to\RR$ ist {\bf Regelfunktion}, falls es eine Folge $(f_n)\subset T[a,b]$
gibt mit $f=\glim f_n$. Die Menge aller Regelfunktionen auf $[a,b]$ wird als $R[a,b]$ bezeichnet.
\end{defin}
Insbesondere ist jede Treppenfunktion auch Regelfunktion.

\begin{defin}\label{int-regel}
Für $f\in R[a,b]$ mit $f=\glim_{n\to\infty} f_n$ für $(f_n)\subset T[a,b]$
definieren wir
\[
\int_a^b f=\lim_{n\to\infty} \int_a^b f_n.
\]
\end{defin}
Damit diese Definition überhaupt Sinn macht, braucht man noch zwei wichtige Schritte, die im folgenden Lemma gemacht werden:
\begin{lemma}
In Definition~\ref{int-regel}, der Grenzwert $\lim_{n\to\infty} \dint_a^b f_n$
existiert und ist von der Wahl der $f_n$ unabhängig.
\end{lemma}
\begin{proof}
Zuerst zeigen wir die Existenz des Grenzwerts. Für $x\in[a,b]$ und $m,n\in\NN$ haben wir
\[
\big|f_m(x)-f_n(x)\big|=\big|f_m(x)-f(x)+f(x)-f_n(x)\big|\le \big|f_m(x)-f(x)\big|+\big|f_n(x)-f(x)\big|,
\]
und dann
\[
\sup_{x\in[a,b]}\big|f_m(x)-f_n(x)\big|\le \sup_{x\in[a,b]}\big|f_m(x)-f(x)\big|+\sup_{x\in[a,b]}\big|f_n(x)-f(x)\big|.
\]
Sei $\varepsilon>0$. Dank $f=\glim_{n\to\infty} f_n$ können wir ein $N\in\NN$ finden mit $\sup_{x\in[a,b]}\big|f_n(x)-f(x)\big|\le\varepsilon$
für alle $n\ge N$. Es folgt daraus, dass für alle $m,n\ge N$ gilt
\[
\sup_{x\in[a,b]}\big|f_m(x)-f_n(x)\big|\le 2\varepsilon.
\]
Sei $I_n:=\dint_a^b f_n$, dann hat man für alle $m,n\ge N$:
\[
|I_m-I_n|=\Big|\int_a^b(f_m-f_n)\Big|\le \sup_{x\in[a,b]}\big|f_m(x)-f_n(x)\big| (b-a)\le 2\varepsilon (b-a).
\]
Da $\varepsilon>0$ beliebig ist, folgt es, dass $(I_n)$ eine Cauchy-Folge in $\RR$ ist, und damit ist sie konvergent (da $\RR$ vollständig ist).


Sei jetzt $(g_n)\subset T[a,b]$ eine beliebiege Folge mit $f=\glim_{n\to\infty} g_n$. Für $J_n:=\dint_a^b g_n$
müssen wir zeigen, dass $\lim J_n=\lim I_n$. Für $x\in[a,b]$ und $m,n\in\NN$ haben wir
\[
\big|f_n(x)-g_n(x)\big|=\big|f_n(x)-f(x)+f(x)-g_n(x)\big|\le \big|f_n(x)-f(x)\big|+\big|g_n(x)-f(x)\big|,
\]
und dann
\[
\sup_{x\in[a,b]}\big|f_n(x)-g_n(x)\big|\le \sup_{x\in[a,b]}\big|f_n(x)-f(x)\big|+\sup_{x\in[a,b]}\big|g_n(x)-f(x)\big|.
\]
Sei $\varepsilon>0$, dann gibt es ein $N\in\NN$ mit $\sup_{x\in[a,b]}\big|f_n(x)-f(x)\big|\le\varepsilon$
und $\sup_{x\in[a,b]}\big|g_n(x)-f(x)\big|\le\varepsilon$ für alle $n\ge N$, und dann auch
\[
|I_n-J_n|=\Big|\int_a^b(f_n-g_n)\Big|\le \sup_{x\in[a,b]}\big|g_n(x)-f(x)\big| (b-a) \text{ für alle $n\ge \NN$},
\]
also $\lim_{n\to\infty} (I_n-J_n)=0$. Daher
\[
\lim_{n\to\infty} J_n=\lim_{n\to\infty} I_n+\lim_{n\to\infty} (I_n-J_n)=\lim_{n\to\infty} I_n.\qedhere
\]
\end{proof}


\begin{bsp}
Wir betrachten jetzt $f:[0,1]\to \RR$ mit $f(x)=x$. Wir wollen zeigen, dass dies eine Regelfuntion ist, danach berechnen wir das Integral $\dint_0^1 f$.

Da $f$ sehr explizit ist, werden auch sehr explizite approximierende Treppenfunktionen konstruieren.
Für $n\in\NN$ definieren wir Treppenfunktionen
$f_n:[0,1]\to\RR$ wie folgt:
\[
f_n(x)=\begin{cases}
0, & x=0,\\
\frac{j}{n}, & x\in\big(\frac{j-1}{n}, \frac{j}{n}], \quad j=1,\dots, n,
\end{cases}
\]
dann
\[
f_n(x)-f(x)=\begin{cases}
0, & x=0,\\
\frac{j}{n}-x, & x\in\big(\frac{j-1}{n}, \frac{j}{n}], \quad j=1,\dots, n.
\end{cases}
\]
Für $x\in\big(\frac{j-1}{n}, \frac{j}{n}]$ gilt $0\le \frac{j}{n}-x\le\frac{1}{n}$, daher
\[
\sup_{x\in[0,1]}\big|f_n(x)-f(x)\big|\le \dfrac{1}{n},
\]
und $f=\glim_{n\to\infty} f_n$. Das zeigt, dass $f$ eine Regelfuntion ist.

Wir weren jetzt $\dint_0^1 f$ berechnen. Für die Treppenfunktionen $f_n$ hat man:
\[
\int_0^1 f_n=\sum_{j=1}^n \dfrac{j}{n} \Big(\frac{j}{n} - \frac{j-1}{n}\Big)=\sum_{j=1}^n \dfrac{j}{n}\,\dfrac{1}{n}
=\dfrac{1}{n^2} \sum_{j=1}^n=\dfrac{1}{n^2}\, n(n+1),
\]
und
\[
\int_0^1 x\, dx=\int_0^1 f=\lim_{n\to+\infty}\int_0^1 f_n=\lim_{n\to\infty}\dfrac{1}{n^2}\, n(n+1)=\dfrac{1}{2}.
\]
\end{bsp}


\begin{satz}\label{satz111}
Stetige Funktionen sind Regelfunktionen.
\end{satz}

\begin{proof}
Seit $f:[a,b]\to\RR$ stetig. In der VL Analysis 1 wurde es gezeigt, dass $f$ auch gleichmässig stetig ist, d.h.
zu jedem $\varepsilon>0$ kann man ein $\delta>0$ finden sodass $\big|f(x)-f(y)\big|\le \varepsilon$
für alle $x,y\in[a,b]$ mit $|x-y|<\delta$.

Sei also $\varepsilon>0$, dann können wir ein $n\in\NN$ finden mit $\big|f(x)-f(y)\big|\le \varepsilon$
für alle $x,y\in[a,b]$ mit $|x-y|<\frac{b-a}{n}$. Wir setzen $x_j:=a+\dfrac{j}{n}\,(b-a)$ für $j=0,1,\dots,n$
und definieren ein Treppenfunktion $f_n$ durch
\[
f_n(x)=\begin{cases}
f(x_j),& x=x_j, \quad j=0,\dots,n,\\
f(\xi_j),& x\in(x_{j-1},x_j), \quad j=1,\dots,n,
\end{cases}
\]
wobei $\xi_j$ ein beliebiger Punkt aus $[x_{j-1},x_j]$ ist.
Für $x\in(x_{j-1},x_j]$ hat man $|\xi_j-x|\le \dfrac{b-a}{n}$, daher $\big|f_n(x)-f(x)\big|=\big|f(x_j)-f(x)\big|\le\varepsilon$.
Also $\sup_{x\in[a,b]} \big|f(x_j)-f(x)\big|\le\varepsilon$. Da $\varepsilon>0$ beliebig ist, gilt
$f=\glim_{n\to\infty}f_n$, damit ist $f$ eine Regelfunktion, und
\[
\int_a^b f=\lim_{n\to\infty} \int_a^b f_n=\lim_{n\to\infty} \sum_{j=1}^n f(\xi_j) (x_{j}-x_{j-1}). \qedhere
\]
\end{proof}


Die Konstruktion des Beweises kann man ein bisschen verallgemeinern:
\begin{defin}
Eine Funktion $f\in[a,b]$ ist {\bf stückweise stetig}, falls es $n\in\NN$ und 
Punkte $x_0,x_1,\dots, x_n\in[a,b]$ gibt, die folgende Eigenschaften erfüllen:
\begin{itemize}
\item $a=x_0<x_1<\dots<x_n=b$,
\item $f$ ist stetig auf jedem der Intervalle $(x_{j-1},x_j)$,  $j=1,\dots,n$,
\item Es existieren die einseitigen Grenzwerte $\lim_{x\to x_j^+}f(x)$, $j=0,\dots,n-1$
und $\lim_{x\to x_j^-}f(x)$, $j=1,\dots,n$.
\end{itemize}
\end{defin}

Mit ein bisschen mehr Arbeit (=Gleichmässige Approximation von $f$ durch Treppenfunktionen auf jedem Teilintervall $(x_{j-1},x_j)$, diese Treppenfunktionen
werden dann zu Treppenfunktion auf dem ganzen Interval $[a,b]$ zusammengeklebt, die $f$ auf $[a,b]$ gleichmässig approximiert)
kann man den Satz~\ref{satz111} auf diese neue Klasse von Funktionen erweitern:
\begin{korol}
Stückweise stetige Funktionen sind Regelfunktionen.
\end{korol}

Durch einfache Anwendung der Grenwertregeln kann man auch folgendes zeigen (wird teilweise im Übungsblatt behandelt):
\begin{satz}
Alle Aussagen vom Lemma~\ref{lem-inti} gelten für $R[a,b]$ statt $T[a,b]$.
\end{satz}

Daraus folgt (Übungsblatt!):

\begin{korol}
Seien $f_n$ Regelfunktionen auf $[a,b]$, die gleichmässig gegen eine Funktion $f:[a,b]\to\RR$ konvergieren,
dann ist auch $f$ Regelfunktion mit $\dint_a^b f=\lim_{n\to\infty}\dint_a^n f_n$.
\end{korol}

Man kann sich fragen, ob es überhaupt Funktionen existieren, die keine Regelfunktionen sind. Das wird im nächsten Beispiel erledigt:

\begin{bsp}
Die Dirichlet-Funktion $f$,
\[
f:[0,1]\to\RR,\quad f(x)=\begin{cases}
0, & \text{für irrationale $x$},\\
1, & \text{für rationale $x$},
\end{cases}
\]
ist keine Regelfunktion.

Sei $g$ eine Treppenfunktion und $(c,d)\subset[0,1]$ ein Interval, auf dem $g$ konstant ist, z.B. $g(x)=A$
für $x\in(c,d)$. Es existieren also ein irrationales $y\in (c,d)$ und ein rationales $z\in (c,d)$,
und wir haben $f(y)=0$ und $f(z)=1$. Mann hat also die Ungleichungen
\[
\sup_{x\in[a,b]} \big[f(x)-g(x)\big|\ge \max\big\{ \big[f(y)-g(y)\big|, \big[f(z)-g(z)\big|\big\}=\max\{ |A|, |1-A|\big\}\ge \dfrac{1}{2}.
\]
Also gilt für \emph{jede} Treppenfunktion $g$: $\sup_{x\in[a,b]} \big[f(x)-g(x)\big|\ge \frac{1}{2}$, deswegen
kann keine Folge von Treppenfunktionen gegen $f$ konvergieren.

Es folgt, dass das Integral $\dint_a^b f$ für diese $f$ mit den vorhandenen Mitteln nicht definiert ist. Wir werden später sehen (Analysis III), dass man die Konstruktion des Integrals erweitern kann,
um dieses Integral von $f$ doch zu definieren.
\end{bsp}

\begin{bmk}
Wir jetzt nochmals die geometrische Bedeutung des Integrals $\dint_a^b f$ angehen. Falls $f$ eine Regelfunktion ist,
dann kann man zu jedem $\varepsilon>0$ eine Treppenfunktion $f_\varepsilon$ finden mit $\big|f_\varepsilon(x)-f(x)\big|\le \varepsilon$.
Es folgt daraus, dass man den Graphen von $f$ durch den Graphen von $f_\varepsilon$ approximieren kann:
\[
\includegraphics[width=60mm]{fig6.eps},
\]
und auch können die Mengen
\[
\begin{aligned}
\Omega_+&:=\{(x,y): a\le x\le b,\, 0\le y\le f(x)\},\\
\Omega_-&:=\{(x,y): a\le x\le b,\, f(x)\le y\le 0\},
\end{aligned}
\parbox{70mm}{\includegraphics[width=65mm]{fig5.eps}}
\]
durch passende Treppenfiguren approximiert werden.

Falls man sich an jetzt an die geometrische Bedeutung des Integrals für Treppenfunktionen erinnert (Bemerkung~\ref{bmk19})
sieht man, dass
\[
\int_a^b f=|\Omega_+|-|\Omega_-|.
\]
\end{bmk}

\newpage

\begin{center}
\large\bf Vorlesung 3
\bigskip
\end{center}

In dieser Vorlesung möchten wir besser verstehen, wie $\dint_a^b f$ von $a$ und $b$ abhängt. Dadurch wird auch eine Berechnungsmethode
für bestimmte Integrale entstehen.


Bislang haben wir $\dint_a^b$ nur für $a<b$ definiert. In vielen Situationen ist es bequem, auch den Fall $a\ge b$ einzuschliessen:
\begin{itemize}
\item für $a=b$ setzen wir $\dint_a^b f:=0$,
\item für $a>b$, falls $f$ Regelfunktion auf $[b,a]$, setzen wir $\dint_a^b f:=-\int_b^a f$.
\end{itemize}
Dieses orientierte Integral behält die wichtigsten Eigenschaften des Integrals,
z.B. gelten alle Identitäten und Abschätzungen vom Lemma~\ref{lem-inti}.
Insbesondere
\[
\text{für $|f|\le c$ gilt $\Big|\dint_a^b f\Big|\le c |b-a|$.}
\]

\begin{satz}[Additivität des Integrals]
Seien $f$ Regelfunktion auf einem Intervall $I$ und $a,b,c\in I$, dann gilt
\begin{equation}
   \label{fabc}
\int_a^b f  +\int_b^c f=\int_a^c f.
\end{equation}
\end{satz}

\begin{proof}
Zuerst betrachten wir den Fall $a<b<c$.

Wir beweisen die Aussage zuerst für Treppenfunktion.
Sei $f\in T[a,c]$, dann gibt es $n\in\NN$ und $a=x_0<x_1<\dots<x_n=c$ mit $f(x)=A_j$ für alle $x\in(x_{j-1},x_j)$,
$j=1,\dots, n$. Wir können voraussetzen, dass  $x_k=b$ für ein $k$ gilt. (Sonst finden wir ein $k$ mit $x_k<b<x_{k+1}$
und betrachen die Verfeinerung $a=y_0<y_1<\dots<y_k<y_{k+1}<y_{k+2}<\dots<y_{n+1}=b$ mit
\[
y_j=\begin{cases}
x_j & \text{falls } j\le k,\\
b, & \text{falls } j={k+1},\\
x_{j-1}, & \text{falls } j\ge k+2,
\end{cases}
\]
diese neue Zerlegung erfüllt die Bedingung.)
Dann hat man eine Zerlegung von $[a,b]$, $a=x_0<\dots<x_k=b$ mit $f(x)=A_j$ für $x\in(x_{j-1},x_j)$, $j=1,\dots, k$,
und eine Zerlegung von $[b,c]$, $b=x_{k}<\dots<x_n=c$ mit $f(x)=A_j$ für $x\in(x_{j-1},x_j)$, $j=k+1,\dots, n$.
Es gilt also
\begin{align*}
\int_a^b f + \int_b^c f &=\sum_{j=1}^k A_j (x_{j}-x_{j-1}) + \sum_{j=k+1}^n A_j (x_{j}-x_{j-1})\\
&=\sum_{j=1}^n A_j (x_{j}-x_{j-1})=\int_a^c f.
\end{align*}
Dadurch ist \eqref{fabc} für die Treppenfunktionen bewiesen.

Sei jetzt $f\in R[a,c]$, dann gibt es eine Folge $(f_n)\subset T[a,c]$ mit $\lim_n\sup_{x\in[a,c]}|f_n(x)-f(x)|=0$.
Es folgt, dass $\lim_n\sup_{x\in[a,b]}|f_n(x)-f(x)|=0$ und $\lim_n\sup_{x\in[b,c]}|f_n(x)-f(x)|=0$.
Das bedeutet, dass
\[
\int_a^b f=\lim_n \int_a^b f_n, \quad \int_b^c f=\lim_n \int_b^c f_n, \quad \int_a^c f=\lim_n \int_a^c f_n.
\]
Da $f_n$ Treppenfunktionen sind, haben wir $\dint_a^b f_n  +\dint_b^c f_n=\dint_a^c f_n$ für alle $n$.
Also
\begin{align*}
\int_a^c f&=\lim_n \int_a^c f_n=\lim_n \Big(\int_a^b f_n  +\int_b^c f_n\Big)\\
&=\lim_n \int_a^b f_n+\lim_n\int_b^c f=\int_a^b f  +\int_b^c f.
\end{align*}
Damit ist der Satz für $a<b<c$ bewiesen.

Alle anderen Fälle muss man jetzt einzeln betrachten und auf den vorherigen Fall reduzieren.
Zum Beispiel, für $a=b$ gilt $\dint_a^bf=0$, und die Aussage \eqref{fabc}
is trivial. Für $b<a<c$ beginnt man mit
\[
\int_{b}^a f+\int_a^c f=\int_b^c f \quad \Leftrightarrow \quad
\int_a^c f=\int_b^c f-\int_{b}^a f,
\]
und man muss nur die Definition $\dint_a^b f= -\dint_{b}^a f$ einsetzen. Analog beweist man die restlichen Fälle.
\end{proof}
Der Satz wird oft in der äquivalenten Form $\dint_a^c f  -\dint_a^b f=\dint_b^c f$ genutzt.



Jetzt kommen wir zur Berechnung der bestimmten Integralen. Am Anfang wollten wir Stammfunktionen ausrechnen können, und
dieser Zusammenhang wird im folgenden Satz begründet. Das ist einer der wichstigsten Sätze der Analysis (und der ganzen Mathematik).

\begin{satz}[Hauptsatz der Differential- und Integralrechnung]\label{hsatz}
Sei $f$ Regelfunktion auf einem Intervall $I$ und $a\in I$. Betrachte die Funktion
\[
\Phi:I\ni x\mapsto \int_a^x f(t)\, dt \in\RR.
\]
Sei $x_0\in I$ und $f$ \emph{stetig} in $x_0$, dann ist $\Phi$ in $x_0$ differenzierbar mit $\Phi'(x_0)=f(x_0)$.
\end{satz}

\begin{proof}
Sei $h\in\RR$, $h\ne 0$, dann gilt
\[
\Phi(x_0+h)-\Phi(x_0)=\int_a^{x_0+h} f(t)\, dt-\int_a^x f(t)\, dt=\int_{x_0}^{x_0+h} f(t)\,dt,
\]
und
\[
\dfrac{\Phi(x_0+h)-\Phi(x_0)}{h}-f(x_0)=\dfrac{1}{h} \, \int_{x_0}^{x_0+h} f(t)\,dt - f(x_0).
\]
Mit Hilfe von
\[
f(x_0) = \dfrac{1}{h} \dint_{x_0}^{x_0+h}f(x_0)\, dt
\]
kann man es wie folgt umschreiben:
\[
\dfrac{\Phi(x_0+h)-\Phi(x_0)}{h}-f(x_0)=\dfrac{1}{h} \, \int_{x_0}^{x_0+h} \big(f(t)-f(x_0)\big)\,dt.
\]
Sei $\varepsilon>0$. Da $f$ in $x_0$ stetig ist, gibt es ein $\delta>0$ mit $\big|f(t)-f(x_0)\big|<\varepsilon$ für $|t-x_0|<\delta$.
Für $|h|\le\delta$ und alle $t$ zwischen $x_0$ und $x_0+h$ hat man dann $\big|f(t)-f(x_0)\big|<\varepsilon$,
\[
\Big|
\int_{x_0}^{x_0+h} \big(f(t)-f(x_0)\big)\, dt
\Big|\le \varepsilon h, \quad 0<|h|\le\delta.
\]
Es folgt, dass
\[
\bigg|\dfrac{\Phi(x_0+h)-\Phi(x_0)}{h}-f(x_0)\bigg|\le \varepsilon \text{ für alle $h$ mit $0<|h|<\delta$},
\]
und damit ist der Satz bewiesen.
\end{proof}

Der Satz hat einige unmittelmare Korollare:

\begin{korol}\label{hkorol1}
Sei $I\subset\RR$ ein Intervall, $a\in I$, und $f:I\to \RR$ stetige Funktion, dann ist $I\ni x\mapsto \dint_a^x f(t)\, dt\in\RR$ eine Stammfunktion
von $f$. 
\end{korol}

\begin{proof}
Wähle $a\in I$ und setze $\Phi(x):=\dint_a^x f(t)\, dt$. Nach dem Satz~\ref{hsatz}
hat man $\Phi'(x)=f(x)$ für alle $x\in I$, da $f$ in $x$ stetig ist.
\end{proof}


\begin{korol}[Newton-Leibniz-Formel]\label{hkorol2}
Sei $I\subset\RR$ Intervall, $a,b\in I$, $f:I\to \RR$ stetige Funktion und $F$ eine beliebige Stammfunktion von $f$ (existiert nach Korollar~\ref{hkorol1}).
Dann gilt
\[
\dint_a^b f(t)\, dt=F(b)-F(a).
\]
\end{korol}

\begin{proof}
Da wir schon eine Stammfunktion $\Phi:x\mapsto \dint_a^x f(t)\, dt$ haben, gibt es eine Konstante $C$ mit $F(x)=\Phi(x)+C$ für alle $x\in I$.
Also
\[
F(b)-F(a)=\int_a^b f(t)\, dt + C -\Big(\underbrace{\int_a^a f(t)\, dt}_{=0} + C\Big)=\int_a^b f(t)\, dt. \qedhere
\]

\begin{bmk}
Die Differenz $F(b)-F(a)$ bezeichnet man oft $F|_a^b$ oder $F(x)|_{a}^{b}$ oder, ganz ausführlich, $F(x)|_{x=a}^{x=b}$.
\end{bmk}

\end{proof}


Die Methoden für die Berechnung von Stammfunktionen (Vorlesung 1) kann man jetzt auf die Berechnung der bestimmten Integrale direkt übertragen.
Damit wir dabei keine Konstanten verlieren, werden wir die Konsequenzen als einzelne Sätze formulieren.

\begin{satz}[Partielle Integration für bestimmte Integrale]
Seien $f,g:I\to \RR$ stetig differenzierbar und $a,b\in I$, dann 
\[
\int_a^b f'g = fg|_{a}^{b}-\int_a^b f g'.
\]
\end{satz}

\begin{proof}
Betrachte die Funktion
\[
\Phi(x)=f(x)g(x)-\int_a^x f g', 
\]
dann
\[
\Phi'(x)=(fg)'(x)-\Big(\int_a^x f g'\Big)'=f'(x)g(x)+f(x)g'(x)-f(x)g'(x)=f'(x)g(x).
\]
Also ist $\Phi$ eine Stammfunktion von $f'g$,
und (Korollar \ref{hkorol2}):
\begin{align*}
\int_a^b f'g&= \Phi(b)-\Phi(a)=f(b)g(b)-\int_a^b f g' - \Big(f(a)g(a)-\underbrace{\int_a^a f g'}_{=0}\Big) \\
&=f(b)g(b)-f(a)g(a)-\int_a^b f g'=fg|_{a}^b -\int_a^b f g'. \qedhere
\end{align*}
\end{proof}

\begin{satz}[Substitution für bestimmte Integrale]
Seien $f:I\to \RR$, $a,b\in I$ und $\varphi:I\to \RR$ stetig differenzierbar, dann 
\[
\int_a^b f\big(\varphi(t)\big)\varphi'(t)\, dt = \int_{\varphi(a)}^{\varphi(b)} f.
\]
\end{satz}

\begin{proof}
Sei $c\in \varphi(I)$.
Betrachte die Funktion
\[
F: \varphi(I)\ni x\mapsto\int_c^{x} f\in\RR.
\]
Nach den obigen Konstruktionen ist $F$ Stammfunktion von $f$, und nach der Kettenregel ist
$\Phi:=F\circ \varphi$ Stammfunktion von $(f\circ \varphi)\, \varphi'$. Daher gilt
\[
\int_a^b f\big(\varphi(t)\big)\varphi'(t)\, dt=\Phi(a)-\Phi(b)=\int_c^{\varphi(b)} f - \int_c^{\varphi(a)}f=\int_{\varphi(a)}^{\varphi(b)} f. \qedhere
\]
\end{proof}

Mit diesen Mitteln kann man viele Integrale berechnen.

\begin{bsp}\label{bsp-41} Berechne $\dint_{0}^1 \sqrt{1-x^2}\, dx$.

Wir möchten jetzt die Substitution $x=\varphi(t)=\sin t$ nutzen. Wir haben also $dx=\varphi'(t)\, dt=\cos t\, dt$,
der Wert $x=0$ entspricht $t=0$ und der Wert $x=1$ entspricht $t=\frac{\pi}{2}$.
Für $t\in[0,\frac{\pi}{2}]$ gilt $\sqrt{1-\sin^2 t}=\cos t$, also 
\begin{align*}
\dint_{0}^1 \sqrt{1-x^2}\, dx&=\dint_{\varphi(0)}^{\varphi(\pi/2)} \sqrt{1-x^2}\, dx\\
&=\int_0^{\pi/2} \sqrt{1-\varphi(t)^2}\varphi'(t)\, dt=\int_0^{\pi/2} \cos^2 t \, dt\\
&=\int_0^{\pi/2} \dfrac{1+\cos(2t)}{2} \, dt= \Big(\dfrac{t}{2}+\dfrac{\sin (2t)}{4}\Big)\Big|_{0}^{\pi/2}\\
&=\dfrac{\pi}{4}+\dfrac{\sin \pi}{4} - \dfrac{0}{2} - \dfrac{\sin 0}{4}=\dfrac{\pi}{4}.
\end{align*}

Bei Rechnungen mit trigonometrischen Funktionen muss man aufpassen. Zum Beispiel merkt man, dass
auch $\varphi(\frac{\pi}{2})=1$ und $\varphi(2\pi)=0$, also
\begin{align*}
\dint_{0}^1 \sqrt{1-x^2}\, dx&=\dint_{\varphi(2\pi)}^{\varphi(\pi/2)} \sqrt{1-x^2}\, dx=\int_{2\pi}^{\pi/2} \sqrt{1-\varphi(t)^2}\varphi'(t)\, dt
\end{align*}
aber 
\[
\int_{2\pi}^{\pi/2} \sqrt{1-\varphi(t)^2}\varphi'(t)\, dt\ne \int_{2\pi}^{\pi/2} \cos^2 t\, dt,
\]
da die Identität $\sqrt{1-\sin^2 t}=\cos t$ nur für $\cos t\ge 0$ gilt (z.B. $\sqrt{1-\sin^2 t}=-\cos t$
für $t\in [\pi/2,\pi]$).
\end{bsp}


Wie schon vorher erwähnt, ist die Definition des bestimmten Integrals durch die Berechnung von Flächeninhalten
motiviert.  Diesen Zusammenhang können wir noch einmal wie folgt zusammenfassen:
\begin{satz}\label{satz-fli}
Seien $f,g$ Regelfunktionen auf $[a,b]$ mit $g(x)\le f(x)$ für alle $x\in[a,b]$, dann 
gilt für den Flächeninhalt $|\Omega|$ der Figur
\[
\Omega=\big\{ (x,y): \quad a\le x\le b, \quad g(x)\le y\le f(x)\big\},
\]
die Formel $|\Omega|=\dint_a^b \big(f(x)-g(x)\big)\, dx$ gegeben.
\end{satz}
Da wir jetzt einige Berechnungsmethoden für bestimmte Integrale haben, können wir damit Flächeninhalte
von einigen geometrischen Figuren ausrechnen.

\begin{bsp} Sei $A=(x_0,y_0)\in\RR^2$ und $R>0$. Die Figur
\[
\Omega=\big\{(x,y): (x-x_0)^2+(y-y_0)^2\le R^2\big\}
\]
ist die Kreisscheibe mit Radius $R$ und Mittelpunkt $A$: nach dem Satz von Pythagoras
ist der Abstand zwischen den Punkten $B=(x,y)$ und $A=(x_0,y_0)$
durch $\sqrt{(x-x_0)^2+(y-y_0)^2}$ gegeben, deswegen
\[
\Omega=\big\{B\in\RR^2:\, \text{Abstand zwischen $B$ und $A$ ist $\le R$}\big\}.
\]
Wir möchten jetzen den Flächeninhalt von $\Omega$ ausrechnen. Die obige Definition von $\Omega$
kann man auch wie folgt umschreiben:
\begin{align*}
\Omega=\big\{(x,y): & \quad x_0-R\le x\le x_0+R,\\
& \quad y_0-\sqrt{R^2-(x-x_0)^2}\le y_0+\sqrt{R^2-(x-x_0)^2}\big\},
\end{align*}
und damit sind wir im Rahmen des Satzes~\ref{satz-fli}:
\[
|\Omega|=2\int_{x_0-R}^{x_0+R} \sqrt{R^2-(x-x_0)^2}\, dx.
\]
Wir substituieren $x=x_0+Rt$, dann entsprechen die Werte $x=x_0\pm R$ den Werten $t=\pm 1$ und $dx= R\, dt$. Also
\begin{align*}
|\Omega|=2\int_{x_0-R}^{x_0+R} \sqrt{R^2-(x-x_0)^2}\, dx=2\int_{-1}^1 \sqrt{R^2-R^2 t^2}\, R\, dt=2R^2\int_{-1}^1\sqrt{1-t^2}\, dt.
\end{align*}
Da wir eine gerade Funktion
auf einem symmetrischen Intevrall integieren,
gilt (Übungblatt 2)
\[
|\Omega|=4R^2\int_0^1\sqrt{1-t^2}\, dt=\pi R^2.
\]
(Das letzte Integral wurde schon im Beispiel~\ref{bsp-41} ausgerechnet.)
\end{bsp}


Mit Hilfe des Integrals kann man einige bekannte Formeln anders umschreiben.

\begin{satz}[Taylorformel mit Integralrestglied]
Seien $I\subset\RR$ ein Intervall, $n\in\NN_0$ und $f:I\to\RR$ eine $(n+1)$-mal stetig differenzierbare Funktion.
Dann gilt für alle $x,x_0\in I$:
\begin{equation}
 \label{tayfrn} 
f(x)=\sum_{k=0}^n \dfrac{f^{(k)}(x_0)}{k!}(x-x_0)^k+R_n(x),
\quad
R_n(x,x_0)=\dfrac{1}{n!}\int_{x_0}^x (x-t)^n f^{(n+1)}(t)\, dt.
\end{equation}
\end{satz}

\begin{proof}
Den Term $R_0$ kann man direkt ausrechnen:
\[
R_0(x,x_0)=\int_{x_0}^x f'(t)\, dt=f(x)-f(x_0),
\]
damit ist die Formel für $n=0$ bewiesen.
Für $n\ge 1$ kann man partiell integrieren:
\begin{multline*}
R_n(x,x_0)=\dfrac{1}{n!}\int_{x_0}^c (x-t)^n f^{(n+1)}(t)\, dt=\dfrac{1}{n!}\Big( (x-t)^n f^{(n)}(t)\Big|_{t=x_0}^{t=x}\\
- \int_{x_0}^x \big(-n (x-t)^{n-1}\big)f^{(n)}(t)\Big).
\end{multline*}
Mit Hilfe von
\[
(x-t)^n f^{(n)}(t)\Big|_{t=x_0}^{t=x}=-(x-x_0)^n f^{(n)}(x_0)
\]
bekommt man die Identität $R_n(x,x_0)+(x-x_0)^n f^{(n)}(x_0)=R_{n-1}(x,x_0)$, und durch Induktion
\begin{align*}
R_n(x,x_0)+\sum_{k=1}^n \dfrac{f^{(k)}(x_0)}{k!}(x-x_0)^k&=R_0(x,x_0)=f(x)-f(x_0). \qedhere
\end{align*}
\end{proof}

Die ``neue'' Taylorsche Formel kann mit der ``alten'' (Analysis I) verglichen werden,
\[
f(x)=\sum_{k=0}^n \dfrac{f^{(k)}(x_0)}{k!}(x-x_0)^k+\dfrac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{n+1},
\]
wobei $\xi$ zwischen $x$ und $x_0$ liegt: die beiden Restglieder müssen natürlich übereinstimmen,
\[
R_n(x)=\dfrac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{n+1} \text{ für ein $\xi$ zwischen $x$ und $x_0$}.
\]
Diese Darstellung kann man aber auch direkt aus allgemeinen Eigenschaften des Integrals
herleiten. Dafür beweisen wir einen anderen wichtigen Satz:
\begin{satz}[Mittelwertsatz für Integrale]\label{mitint}
Seien $f,g:[a,b]\to \RR$ stetig, wobei $g\ge 0$. Dann existiert ein $\xi\in[a,b]$ mit
\begin{equation}
  \label{eqfg}
\int_a^b f g=f(\xi)\int_a^b g.
\end{equation}
\end{satz}
\begin{proof}
Falls $g(x)=0$ für alle $x\in[a,b]$, so sind die beiden Integrale in \eqref{eqfg} gleich Null,
und die Identität ist richtig für alle $\xi\in[a,b]$. Wir können also annehmen, dass
$g$ keine Nullfunktion ist, also $\dint_a^b g>0$ (Übungsblatt 2).
Wir bezeichnen
\[
m=\min_{x\in[a,b]} f(x), \quad M=\max_{x\in[a,b]} f(x),
\]
dann $m g(x)\le f(x)g(x)\le Mg(x)$ für alle $x\in[a,b]$, und
\[
m\int_a^b g\le \dint_a^b f g\le M\dint_a^b g, \quad
m\le\dfrac{\dint_a^b f g}{\dint_a^b g}\le M.
\]
Nach dem Zwischenwertsatz gibt es also ein $\xi\in[a,b]$ mit
\[
\dfrac{\dint_a^b f g}{\dint_a^b g}=f(\xi).\qedhere
\]
\end{proof}

\begin{korol}
Sei $f$ wie in \eqref{tayfrn}, dann gilt für ein $\xi$ zwischen $x_0$ und $x$:
\[
R_n(x,x_0)=\dfrac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{n+1}.
\]
\end{korol}

\begin{proof}
Für $x=x_0$ ist die Aussage trivial. Wir betrachten jetzt $x>x_0$,
dann ist $(x-x_0)^{n}\ge 0$, und nach dem Mittelwertsatz \ref{mitint} gibt es ein $\xi\in[x_0,x]$
mit
\begin{align*}
R_n(x,x_0)&=\dfrac{1}{n!}\int_{x_0}^x (x-t)^n f^{(n+1)}(t)\, dt=\dfrac{1}{n!}\, f^{(n+1)}(\xi)\int_{x_0}^x (x-t)^n \, dt\\
&=\dfrac{1}{n!}\, f^{(n+1)}(\xi) \Big(-\dfrac{(x-t)^{n+1}}{n+1}\Big)\Big|_{t=x_0}^{t=x}=\dfrac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{n+1}.
\end{align*}
Für $x<x_0$ gilt $R_n(x,x_0)=\dfrac{(-1)^{n+1}}{n!}\dint_{x}^{x_0}(t-x)^n f^{(n+1)}(t)\, dt$.
Jetzt hat man $(t-x)^n\ge 0$ für $t\in[x,x_0]$, und nach dem Mittelwertsatz gibt es  ein $\xi\in[x,x_0]$
mit
\begin{align*}
R_n(x,x_0)&=\dfrac{(-1)^{n+1}}{n!}\, f^{(n+1)}(\xi)\dint_{x}^{x_0}(t-x)^n\, dt\\
&=\dfrac{(-1)^{n+1}}{n!}\, f^{(n+1)}(\xi) \dfrac{(t-x)^{n+1}}{n+1}\Big|_{t=x}^{t=x_0}
=\dfrac{(-1)^{n+1}}{n!}\, f^{(n+1)}(\xi) \dfrac{(x_0-x)^{n+1}}{n+1}\\
&=\dfrac{f^{(n+1)}(\xi)}{(n+1)!}(x-x_0)^{n+1}. \qedhere
\end{align*}
\end{proof}
Die Taylorformel aus der Analysis I ist also ein Korollar der neuen Taylorformel mit Integralrestglied.


\newpage

\begin{center}
\large\bf Vorlesung 4
\bigskip
\end{center}

Bisher haben wir das Integral $\dint_a^b f$ unter der Voraussetzung definiert, dass $f$ auf einem endlichen abgeschlossenen Intervall $[a,b]$ Regelfunktion ist (daraus folgt, dass $f$ in jedem Punkt definiert ist und dass $f$ beschränkt ist: siehe unten).
In vielen Fällen ist es aber wichtig, auch allgemeinere Intevralle und Funktionen zu betrachten,
z.B. $b-a=\infty$ und/oder $f$ nur auf $(a,b)$ definiert ist (und daher kann $f$ unbeschränkt sein).
Die entsprechende Konstruktion nennt man \emph{uneigentliches Integral}.

In dieser Vorlesung werden wir folgende Abkürzung nutzen: Sei $(a,b)\subset \RR$, dann ist eine Funktion $f:(a,b)\to\RR$ \emph{auf $(a,b)$
lokal integrierbar}, falls $f$ auf jedem endlichen abgeschlossenen Teilinterval von $(a,b)$ Regelfunktion ist. (Dann ist das Integral
$\dint_c^d f$ für alle $[c,d]\subset(a,b)$ wohldefiniert.)



\begin{defin}
Seien $a,b\in\overline{\RR}$ (also $-\infty\le a <b\le+\infty)$ und sei $f:(a,b)\to\RR$ eine lokal integrierbare Funktion.
Das \emph{uneigentliche Integral} $\dint_a^b f$ wird durch folgende
Regeln definiert:
\begin{enumerate}
\item Ist $f$ in $a$ definiert und auf jedem Teilinterval $[a,\beta]$ mit $\beta\in(a,b)$ Regelfunktion ist,
sei $\dint_a^b f:=\lim_{\beta\to b^-} \int_a^\beta f$,
\item Ist $f$ in $b$ definiert und auf jedem Teilinterval $[\alpha,b]$ mit $\alpha\in(a,b)$ Regelfunktion ist,
sei $\dint_a^b f:=\lim_{\alpha\to a^+} \int_\alpha^b f$,
\item Im Allgemeinen: wähle $c\in (a,b)$ und definiere $\dint_a^b=\underbrace{\dint_a^c f}_{\text{definiert in 2}}+\underbrace{\dint_c^b f}_{\text{definiert in 1}}$.
\end{enumerate}
Falls die obigen Grenzwerte existieren und endlich sind, sagt man, dass das uneigentliche Integral $\dint_a^b f$ \emph{konvergiert}
und das die Funktion $f$ auf $(a,b)$ \emph{integrierbar} ist, sonst heisst das Integral $\dint_a^b f$ divergiert.
Wir definieren auch $\dint_a^a f:=0$ und $\dint_a^b f=-\dint_b^a f$ für $a>b$.
 \qed
\end{defin}

Als einfache freiwillige Übung kann man zeigen, dass das Ergebnis von der Wahl vom $c$ im Punkt 3 der Definition unabhängig ist. Die folgende Proposition
zeigt, dass das uneigentliche Integral eine Verallgemeinerung des üblichen (``eigentlichen'') Integrals ist, daher ist das Benutzen vom selben Zeichen
$\dint_a^b$ begründet.

\begin{prop}
Sei $[a,b]\subset\RR$ ein endliches Intervall und $f:[a,b]\to\RR$ eine Regelfunktion, dann gilt
\[
\lim_{\beta\to b^-} \int_a^\beta f = \dint_a^b f = \lim_{\alpha\to a^+} \int_\alpha^b f
\]
\end{prop}

\begin{proof}
Wir beweisen nur die erste Gleichheit (die zweite beweist man analog).
Wir werden zuerst zeigen, dass $f$ beschränkt ist. Sei $f_n$ eine Folge von Treppenfunktionen auf $[a,b]$,
die gleichmässig auf $[a,b]$ gegen $f$ konvergiert. Dann findet man ein $N\in\NN$ mit $\big|f_N(x)-f(x)\big|\le 1$ für alle $x\in[a,b]$.
Jede Treppenfunktion ist beschränkt, also existiert ein $C>0$ mit $|f_N(x)|\le C$ für alle $x\in[a,b]$. Dann gilt für alle
$x\in[a,b]$:
\[
\big|f(x)\big|=\big|f(x)-f_N(x) + f_N(x)\big|\le \big|f(x)-f_N(x)\big| + \big|f_N(x)\big|\le 1+C,
\]
also ist $f$ beschränkt. Damit haben wir, zu jedem $\beta\in(a,b)$,
\[
\Big|\dint_a^b f - \int_a^\beta f\Big|=\Big| \int_\beta^b f\Big|\le \int_\beta^b |f| \le (1+C)(b-\beta)\xrightarrow{\beta\to b^-} 0. \qedhere
\]
\end{proof}

Einige Beispiele kann man ganz direkt ausrechnen.
\begin{bsp}[Sehr wichtig, wird später mehrmals verwendet]\label{bsx}
Wir untersuchen zuerst die Konvergenz des Integrals
$\dint_1^{+\infty} \dfrac{1}{x^s}\, dx$ mit $s\in\RR$.
Die Funktion ist auf jedem Teilintevral $[1,b]$ stetig (also Regelfunktion), daher
\[
\dint_1^{+\infty} \dfrac{1}{x^s}\, dx=\lim_{b\to+\infty}\dint_1^b \dfrac{1}{x^s}\, dx=\lim_{b\to+\infty } F|_{1}^b=F(+\infty) - F(1),
\]
wobei $F$ eine beliebeige Stammfunktion von $x\mapsto x^{-s}$ ist.
\begin{itemize}
\item Für $s=1$ kann man $F(x)=\ln x$ wählen, man hat also $F(+\infty)=+\infty$,
und das Integral divergiert in diesem Fall.
\item Für $s\ne 1$ nimmt man $F(x)=\dfrac{x^{1-s}}{1-s}$, dann gilt $F(+\infty)=\begin{cases} +\infty, & s<1,\\ 0, & s>1.\end{cases}$
\end{itemize}
Das Integral $\dint_1^{+\infty} \dfrac{1}{x^s}\, dx$ konvergiert also dann und nur dann, wenn $s>1$.

Analog betrachtet man das Integral
\[
\dint_0^1 \dfrac{1}{x^s}\, dx=\lim_{a\to 0^+} \dint_a^1 \dfrac{1}{x^s}\, dx = F(1)-F(0+)
\]
mit derselben Stammfuntion $F$. In diesem Fall gilt aber $F(0+)\in\RR$ genau dann, wenn $s<1$:
Das Integral $\dint_0^1 \dfrac{1}{x^s}\, dx$ konvergiert also dann und nur dann, wenn $s<1$.

Man sieht sofort, dass das Integral $\dint_0^{+\infty} \dfrac{1}{x^s}\, dx=\dint_0^1 \dfrac{1}{x^s}\, dx+\dint_1^\infty \dfrac{1}{x^s}\, dx$ für alle $s\in\RR$ divergiert.
\end{bsp}



Es gibt viele Analogien zwischen uneigentlichen Integralen und Reihen. Man fängt mit dem uneigentlichen Integrieren von nichtnegativen Funktionen an:
\begin{prop}\label{supint}
Sei $f\ge 0$ lokal integrierbar auf $(a,b)$. Das Integral $\dint_a^b f$ konvergiert dann und nur dann, wenn 
\[
\sup_{[\alpha,\beta]\in(a,b)}\int_\alpha^\beta f<\infty.
\]
\end{prop}

\begin{proof}
Wähle $c\in (a,b)$ und bezeichne $F(\alpha)=\dint_\alpha^c f$ und $G(\beta):=\dint_c^\beta f$.
Die Funktion $F$ ist auf $(a,b)$ fallend: für $\alpha\le \alpha'$ gilt
\[
F(\alpha)=F(\alpha')+\int_\alpha^{\alpha'} f\ge F(\alpha'),
\]
und die Existenz des Grenzwerts $\lim_{\alpha\to a^+} F(\alpha)$ ist äquivalent zu $\sup_{\alpha\in(a,b)} F(\alpha)<\infty$.
Analog zeigt man, dass $G$ auf $(a,b)$ steigend ist, also existiert $\lim_{\beta\to b^-} F(\beta)$ genau dann, wenn 
$\sup_{\beta\in(a,b)} G(\beta)<\infty$.
Die Existenz der beiden Grenzwerten ist also äquvalent zu $\sup_{\alpha,\beta\in(a,b)} \big( F(\alpha)+G(\beta)\big)<\infty$,
was man als
\[
\sup_{\alpha,\beta\in(a,b)} \int_a^b f <\infty
\]
umschreiben kann. Wegen $\dint_a^b f\le 0$ für $a>b$ kann man $\sup_{\alpha,\beta\in(a,b)}$
durch $\sup_{[\alpha,\beta]\in(a,b)}$ ersetzen.
\end{proof}

In der Praxis kann man das folgende Kriterium nutzen, das man mit denselben Ideen beweisen kann:
\begin{prop}
Seien $\alpha_n\in (a,b)$ eine gegen $a$ konvergierende Folge und $(\beta_n)\subset (a,b)$ eine gegen $b$ konvergierende Folge.
Sei $f\ge 0$, dann ist $\dint_a^b f$ genau dann konvergent, wenn $\sup_{n\in\NN} \dint_{\alpha_n}^{\beta_n} f<\infty$.
\end{prop}

Auch für Integrale gibt es das Majoratenkriterium für die Konvergenz:

\begin{satz}[Majoratenkriterium für uneigentliche Integrale]
Seien $-\infty\le a<b\le +\infty$ und $f,g$ auf $(a,b)$ lokal integrierbare Funktionen mit $g\ge 0$.
\begin{enumerate}
\item Falls $\big|f(x)\big|\le g(x)$ für alle $x\in(a,b)$ gilt und $\dint_a^b g$ konvergiert, dann konvergiert auch $\dint_a^b f$, dabei gilt
$\Big|\dint_a^b f\Big|\le \dint_a^b g$.
In diesem Fall sagt man, dass $g$ \emph{integrierbare Majorante} ist.

\item Falls $f(x) \ge g(x)$ für alle $x\in(a,b)$ gilt und $\dint_a^b g$ divergiert, dann divergiert auch $\dint_a^b f$.
In diesem Fall sagt man, dass $g$ \emph{nichtintegrierbare Minorante} ist.

\end{enumerate}

Insbesondere gilt: falls $\dint_a^b |f|$ konvergiert, dann konvergiert auch $\dint_a^b f$. In diesem Fall heisst das Integral \emph{absolut konvergent}
und die Funktion $f$ auf \emph{$(a,b)$ absolut integrierbar}.
\end{satz} 


\begin{proof}
(1) Wir betrachten nur den Fall, dass $f$ und $g$ auf $[a,\beta)$ für alle $\beta\in(a,b)$ Regelfunktionen  sind (die anderen Fälle beweist man analog).
Für $\beta\in (a,b)$ definiere 
\[
I(\beta)=\int_a^\beta f, \quad J(\beta)=\int_a^\beta g
\]
und sei $(\beta_n)\subset(a,b)$ eine Folge, die gegen $b$ konvergiert. Dann gibt es zu jedem $\varepsilon>0$
ein $N\in \NN$ mit
\[
\big|J(\beta_m)-J(\beta_n)\big|\equiv \Big|\dint_{\beta_n}^{\beta_m} g\Big|<\varepsilon
\] für alle $m,n\ge N$.
Dann gilt auch
\begin{align*}
\big|I(\beta_m)-I(\beta_n)\big|&=\Big| \int_a^{\beta_m} f - \int_a^{\beta_n} f \Big| =
\Big| \int_{\beta_n}^{\beta_m} f \Big|\le \Big| \int_{\beta_n}^{\beta_m} |f| \Big|\le  \Big|\dint_{\beta_n}^{\beta_m} g\Big|<\varepsilon.
\end{align*}
Es folgt, dass $I(\beta_n)$ eine Cauchy-Folge in $\RR$ ist, dadurch konvergiert sie.
Wir müssen aber noch zeigen, dass der Grenzwert von der Wahl der Folge $(\beta_n)$ unabhängig ist.
Sei also $(\beta_n')\subset(a,b)$  eine weitere Folge, die gegen $b$ konvergiert. Wir haben
 $\lim_{n} J(\beta_n)=\lim_n J(\beta'_n)=\dint_a^b g$, also $\lim_{n} \big|J(\beta_n)-J(\beta_n')\big|=\lim_n \Big| \dint_{\beta'_n}^{\beta_n} g\Big|=0$.
Aus der Abschätzung
\begin{align*}
\big|I(\beta_n)-I(\beta'_n)\big|&=\Big| \int_{\beta'_n}^{\beta_n} f \Big|\le \Big| \int_{\beta'_n}^{\beta_n} |f| \Big|\le  \Big|\dint_{\beta'_n}^{\beta_n} g\Big|
\end{align*}
folgt, dass $\lim_n \big|I(\beta_n)-I(\beta'_n)\big|=0$, d.h. $\lim_{n} I(\beta_n)=\lim_n I(\beta'_n)$, und
\[
\Big| \dint_a^b f\Big|=\Big\| \lim_n \int_a^{\beta_n} f \Big|\le \lim_n \int_a^{\beta_n} |f| \le \lim_n \int_a^{\beta_n} g.
\]

(2) Wir haben $f\ge g\ge 0$. Nehme an, dass das Integral $\dint_a^b f$ konvergiert, dann hat man (Proposition~\ref{supint})
$\sup_{[\alpha,\beta]\subset(a,b)}\dint_\alpha^\beta f<+\infty$, und dann auch $\sup_{[\alpha,\beta]\subset(a,b)}\dint_\alpha^\beta g<+\infty$,
was der Divergenz von $\dint_a^b g$ widerspricht.
\end{proof}


\begin{bsp}\label{dirint}
Wir werden zeigen, dass das \emph{Dirichlet-Integral} $\dint_{0}^{+\infty} \dfrac{\sin x}{x}\, dx$ konvergiert, aber nicht absolut.
Im Punkt $0$ gibt es keine Schwierigkeiten (die Funktion lässt sich stetig in 0 fortsetzen), wir müssen uns also nur
um $\lim_{\beta\to +\infty} \dint_a^\beta$ mit beliebigem $a\ge 0$ kümmern.

Wir beweisen zuerst, dass das Integral nicht absolut konvergent ist.
Sei $n\in\NN$, dann gilt,
\begin{align*}
\int_0^{\pi n} \Big| \dfrac{\sin x}{x}\Big|\, dx&=\sum_{k=1}^n\int_{\pi(k-1)}^{\pi k} \Big| \dfrac{\sin x}{x}\Big|\, dx\ge
\sum_{k=1}^n\int_{\pi(k-1)}^{\pi k} \Big| \dfrac{\sin x}{\pi k}\Big|\, dx\\
&\stackrel{x=\pi(k-1)+y}{=}\sum_{k=1}^n\int_{0}^\pi
\Big| \dfrac{\sin \big(\pi(k-1)+y\big)}{\pi k}\Big|\, dy\\
&=\sum_{k=1}^n\int_{0}^\pi
\Big| \dfrac{\sin y}{\pi k}\Big|\, dy= \dfrac{2}{\pi} \sum_{k=1}^n \dfrac{1}{k}.
\end{align*}
 Da die harmonische Reihe $\sum_n 1/n$ divergiert, gilt
\[
\lim_{n\to+\infty }\int_0^{\pi n} \Big| \dfrac{\sin x}{x}\Big|\, dx\ge \dfrac{2}{\pi} \sum_{k=1}^\infty \dfrac{1}{k}=+\infty,
\]
also ist das Integral divergent.


Jetzt zeigen wir, dass das Integral (nicht absolut) konvergent ist. Das Majorantenkriteriul hilft leider nicht: man kann
zwar $|\sin x / x|\le 1/x$ abschätzen, aber die Majorante $1/x$ ist nicht integrierbar. Wir können aber partiell integrieren:
\begin{align*}
\int_1^R \underbrace{\sin x}_{f'} \underbrace{\dfrac{1}{x}}_{g} &= -\dfrac{\cos x}{x}\Big|_{1}^R - \int_1^R\dfrac{\cos x}{x^2}\, dx=\cos 1 - \underbrace{\dfrac{\cos R}{R}}_{\to 0 \text{ für } R\to+\infty}- \int_1^R\dfrac{\cos x}{x^2}\, dx.
\end{align*}
Die Konvergenz des Dirichlet-Integrals ist damit auf die Konvergenz von $\dint_1^\infty\dfrac{\cos x}{x^2}\, dx$ reduziert.
Dieses neue Integral kann man mit dem Majorantenkriterium untersuchen: $\Big|\dfrac{\cos x}{x^2}\Big|\le \dfrac{1}{x^2}$
und $\dfrac{1}{x^2}$ ist auf $[1,+\infty)$ integrierbar. Also ist das Dirichlet-Integral konvergent. \qedhere
\end{bsp}





Die vorherigen Aussagen kann man kombinieren, um nutzliche Konvergenzekriterien herzuleiten:
\begin{prop}
Sei $a>0$.
\begin{enumerate}
\item Sei $f$ auf $(0,a)$ lokal integrierbare Funktion.
\begin{enumerate}
\item Falls es $M>0$ und $s<1$ existieren mit $|f(x)|\le M/x^s$ für alle $x\in(0,a)$, dann konvergiert $\dint_0^a f$ absolut.
\item Falls es $M>0$ und $s\ge 1$ existieren mit $f(x)> M/x^s$ für alle $x\in(0,a)$, dann divergiert $\dint_0^a f$.
\end{enumerate}
\item Sei $f$  auf $(a,+\infty)$ lokal integrierbare Funktion.
\begin{enumerate}
\item Falls es $M>0$ und $s>1$ existieren mit $|f(x)|\le M/x^s$ für alle $x\in(a,+\infty)$, dann konvergiert $\dint_a^{+\infty} f$ absolut.
\item Falls es $M>0$ und $s< 1$ existieren mit $f(x)> M/x^s$ für alle $x\in(a,+\infty)$, dann divergiert $\dint_a^{+\infty} f$.
\end{enumerate}
\end{enumerate}
\end{prop}

\begin{proof}
(1a) Das Integral $\dint_0^a M/x^s\,dx=M\dint_0^1 1/x^s\,dx+M\dint_1^a 1/x^s\,dx$ konvergiert für $s<1$ (Bespiel~\ref{bsx}),
also folgt die Konvergenz von $\dint_0^a |f|$ mit Hilfe des Majorantenkriteriums.
(1b) Für $s\ge 1$ ist das Integral  $\dint_0^a M/x^s\,dx=M\dint_0^1 1/x^s\,dx+M\dint_1^a 1/x^s\,dx$ divergent (Bespiel~\ref{bsx}),
also folgt die Divergenz von $\dint_0^a f$ aus dem Majorantenkriterium.
Die Aussagen (2a) und (2b) beweist man analog.
\end{proof}

\begin{bsp} Die Analogie zwischen Reihen und uneigentlichen Integralen hat aber auch gewisse Grenzen.
Zum Beispiel, wissen wir, dass für konvergente Reihe $\sum_n a_n$ immer $\lim_n a_n=0$ gilt.
Aus der Konvergenz von $\dint_a^b f$ kann man aber nicht schliessen, dass $\lim_{x\to a^+} f(x)=\lim_{x\to b^-}f(x)=0$ (bzw. ob die Grenzwerte überhaupt existieren):
zum Beispiel liefert die Funktion $x\mapsto x^{-1/2}$ auf $(0,1)$ ein Gegenbeispiel für beschränkte Intevralle $(a,b)$.
Auch für unbeschränkte $(a,b)$ kann man ein ähnliches Beispiel konstruieren. Betrachte z.B. $f:[2,+\infty)\to \RR$,
\[
f(x)=\begin{cases}
n, & x\in\Big[n,n+\dfrac{1}{n^3}\Big] \text{ für ein $n\in\NN$ mit $n\ge 2$,}\\
0, & \text{sonst}.
\end{cases}
\]
Sei $N\in\NN$, $N\ge 3$, dann ist $f$ auf $[2,N]$ Regelfunktion (sogar Treppenfunktion), und
\begin{multline*}
\int_2^N f=\sum_{n=2}^{N-1} \int_n^{n+1} f= \sum_{n=1}^{N-1} \Big(\int_n^{n+1/n^3} \underbrace{f(x)}_{=n}\, dx + \int_{n+1/n^3}^{n+1} \underbrace{f(x)}_{=0}\, dx\Big)\\
=\sum_{n=2}^{N-1} n \cdot \dfrac{1}{n^3}=\sum_{n=2}^{N-1} \dfrac{1}{n^2}.
\end{multline*}
Wir wissen, dass die Reihe $\sum_n 1/n^2$ konvergeirt, daher gilt $\sup_{N\ge 2}\dint_2^N f<\infty$, und das Integral $\dint_2^{+\infty} f$ ist konvergent.
Dabei ist $f$ im Unendlichen unbeschränkt: zu jedem $C>0$ und jedem $N>0$ gibt es ein $x>N$ mit $f(x)>C$. \qed
\end{bsp}

Im vorherigen Beispiel haben wir die Konvergenz eines Integrals mit Hilfe der Konvergenz einer Reiehe bewiesen.
Man kann auch die Konvergenz von Reihen mit Hilfe von Integralen untersuchen, und das ist eine sehr wichtige Beweismethode:

\begin{satz}[Integralkriterium für Reihen]
Sei $f:[1,+\infty)\to\RR$ monoton fallend, nichtnegativ, Regelfunktion auf jedem Teilintervall $[1,R]$, $R>0$. Dann konvergiert
die Reihe $\sum_{k=1}^\infty$ genau dann, wenn das Integral $\dint_1^\infty f$ konvergiert.
\end{satz}

\begin{proof}
Für $x\in [n,n+1]$ gilt $f(n+1)\le f(x)\le f(n)$, also
\[
f(n+1)\le \int_{n}^{n+1} f(x)\, dx \le f(n).
\]
Ist das Integral $\dint_0^\infty f$ konvergent, so gilt zu jedem $N\in\NN$:
\[
\sum_{n=1}^N f(n+1)\le \sum_{n=1}^N \int_{n}^{n+1} f=\int_1^{N+1} f\le \int_1^\infty f<\infty,
\]
also ist $\sum_{n=1}^\infty f(n+1)\equiv \sum_{n=2}^\infty f(n)$ konvergent.

Ist die Reihe $\sum_{n=1}^\infty f(n)$ konvergent, so gilt zu jedem $N\in\NN$:
\[
\int_1^N f= \sum_{n=1}^{N-1} \int_{n}^{n+1} f\le \sum_{n=1}^{N-1} f(n)\le \sum_{n=1}^\infty f(n)<\infty,
\]
also ist auch das Integral konvergent.
\end{proof}


Mit ähnlichen Konstruktionen kann man auch den folgenden Satz beweisen (Übung):
\begin{satz}\label{temp00}
Sei $f:[1,+\infty)\to\RR$ monoton fallend, nichtnegativ, Regelfunktion auf jedem Teilintervall $[1,R]$, $R>0$, dann konvergiert
die Folge
\[
a_{n}=\int_{1}^{n+1} f(x)\,dx -\sum_{k=1}^n f(k).
\]
(Dabei können die Reihe $\sum_{n=1}^\infty f(n)$ und das Integral $\dint_1^\infty f$ divergieren.)
\end{satz}
\begin{bsp}
Sei $s>1$. Die Reihe $\sum_{n=1}^\infty \dfrac{1}{n^s}$ konvergiert genau dann, wenn das Integral $\dint_1^\infty \dfrac{dx}{x^x}$ konvergiert, d.h.
genau dann, wenn $s>1$ (Beispiel \ref{bsx}).

Die Funktion $s\mapsto \zeta(s)=\sum_{n=1}^\infty \dfrac{1}{n^s}$
heisst \emph{Riemannsche Zetafunktion}. Die kann man auf eine bestimmte kanonische Weise auch für bestimmte komplese $s$ definieren,
und die (komplexen) Nullstellen dieser Funktion sind von grosser Bedeutung in vielen Bereichen der Mathematik, und sie sind das Thema der berühmten
Riemannschen Vermutung.\footnote{Die Riemannsche-Vermutung ist ein der ältesten ungelösten Probleme der Mathemaik, und sie gehört zu den sieben Millenium-Problemen der Mathematik, die vom Clay-Institut (USA) im Jahr 2000 formuliert wurden. Für die Lösung jedes Millenium-Problems gibt es ein Preisgeld von einer Million US-Dollar. Bisher wurde nur ein Millenium-Problem gelöst: Im Jahr 2002 hat Grigori Perelman die Poincaré-Vermutung bewiesen (das Preisgeld und
weitere Ehrungen hat er aber abgelehnt).}

Mit Hilfe des Satzen \ref{temp00} sieht man auch, dass die Folge
\[
b_n=\int_1^n\dfrac{1}{x}\, dx -\sum_{k=1}^{n-1} \dfrac{1}{k}=\ln n -\sum_{k=1}^{n-1} \dfrac{1}{k}
\]
konvergiert. Dann existiert auch der Grenzwert
\[
\gamma:=\lim_{n\to +\infty} \big(\sum_{k=1}^{n} \dfrac{1}{k} - \ln n \big)=\lim_{n\to +\infty} \big(\dfrac{1}{n} -b_n \big),
\]
den man als \emph{Euler-Mascheroni-Konstante} bezeichnet: das ist eine der wichtigsten mathematischen Konstanten. Man hat $\gamma\simeq 0,577\dots$.
\end{bsp}

Das da uneigentliche Integral als Grenzwzert von ``eigentlichen'' Integral definert wird, kann man die meisten
Berechnungsmethoden auf den uneigentlichen Fall übertragen. Wir werden nur die entsprechnden Formulierungen angeben, da die Beweise aus
den entsprechnden Formeln für bestimmte Integrale und Grenzübergängen folgen:
\begin{prop}[Partielle Integration für uneigentliche Integrale]
Seien $f,g$ auf $(a,b)$  stetig differenzierbar. Nehme an, dass die beiden Grenzwerte $(fg)(a+)$ und $(fg)(b-)$ existieren.
Dann konvergiert $\dint_a^b f'g $ genau dann, wenn $\dint_a^b fg'$ konvergiert, und im Falle der Konvergenz
hat man
\[
\dint_a^b f'g = (fg)(b-)- (fg)(a+)- \dint_a^b fg'.
\]
Die Differenz $(fg)(b-)- (fg)(a+)$ bezeichnet man oft immer noch als $fg|_{a}^b$.
\end{prop}

\begin{prop}[Substitution für uneigentliche Integrale]\label{upr1}
Seien $g$ auf $(a,b)$ stetig differenzierbar und \emph{streng monoton} und $f$ auf $g\big( (a,b)\big)$ lokal integrierbar,
dann gilt
\[
\int_a^b f\big(g(x)\big)g'(x)\, dx= \int_{g(a+)}^{g(b-)} f\, dx.
\]
(D.h. beide Integrale konvergieren oder divergieren gleichzeitig. Falls sie konvergieren, haben sie denselben numerischen Wert.)
\end{prop}
In Proposition~\ref{upr1}, garantiert die Monotonie von $g$,
dass das Bild $g\big( (a,b)\big)$ auch ein offenes Intervall ist.

\begin{bsp}
Das Integral $\dint_0^\infty \sin (e^x)\, dx$ konvergiert: mit Hilfe der Substitution $x=\ln t$ hat man die Gleichheit
\[
\dint_0^\infty \sin (e^x)\, dx=\int_1^\infty \dfrac{\sin t}{t}\, dt,
\]
und das Integral auf der rechten Seite ist konvergent (schon gesehen im Beispiel~\ref{dirint}).
\end{bsp}

\begin{bsp}
Es gilt $\dint_0^\infty \dfrac{\sin x}{x}\, dx=\dint_0^\infty \Big(\dfrac{\sin x}{x}\Big)^2\, dx$.
Für den Beweis nutzen wir zuerst die Substitution $x=2y$ und danach die partielle Integration:
\begin{align*}
\dint_0^\infty \dfrac{\sin x}{x}\, dx&=\int_0^\infty \dfrac{\sin (2y)}{2y}\, 2 dy = \int_0^\infty \dfrac{2 \sin(y) \cos (y)}{y}\, dy\\
&=\int_0^\infty \dfrac{(\sin^2 y)'}{y}\, dy= \underbrace{\dfrac{\sin^2 y}{ y}\Big|_0^\infty}_{=0}+\dint_0^\infty \Big(\dfrac{\sin y}{y}\Big)^2\, dy.
\end{align*}
\end{bsp}

\newpage

\begin{center}
\large\bf Vorlesung 5
\bigskip
\end{center}

Jetzt befassen wir uns mit Lösungen von Differentialgleichungen.  Wir betrachten in dieser Vorlesung einige Klassen von Gleichungen, die mit Hilfe von Integralen explizit ``gelöst'' werden können. In folgenden Vorlesungen werden wir dann die allgemeine Theorie entwickeln.

Eine \emph{(gewöhnliche) Differentialgleichung erster Ordnung} (DGl) ist eine Gleichung der Form
\begin{equation}
  \label{gdg}
y'(x)=F\big(x,y(x)\big),
\end{equation}
wobei $F$ eine bekannte Funktion von zwei Variablen ist.
Eine Lösung dieser Differentialgleichung ist eine Funktion $y$, die auf einem Intervall $I\subset \RR$ definiert ist und die Gleichung \eqref{gdg}
für alle $x\in I$ erfüllt.

\begin{bsp}
\begin{enumerate}

\item Sei $F(x,y)$ unabhängig von $y$, d.h. es existiert eine Funktion $f$ von $x$ mit $F(x,y)=f(x)$ für alle $x,y$. Die entsprechende DGl \eqref{gdg} ist $y'(x)=f(x)$. Die Menge der Lösungen ist genau die Menge der Stammfunktionen von $f$.

\item Sei $F(x,y)=y$ für alle $x$, $y$. Die entsprechende DGl ist $y'(x)=y(x)$. Eine mögliche Lösung ist $y(x)=e^x$. Allerdings prüft man ganz einfach, dass alle Funktionen $y(x)=ce^x$, wobei $c\in \RR$ eine Konstante ist, Lösungen sind. Wir werden jetzt ``zu Fuss'' beweisen, dass es keine anderen Lösungen gibt. Wir verwandeln die DGl in eine äquivalente Form, indem wir die beiden Seiten durch $e^{-x}$ multiplizieren: $e^{-x}y'(x)=e^{-x}y(x)$, oder
$y'(x)e^{-x} - y(x)e^{-x}=0$, oder $\big(y(x) e^{-x}\big)'=0$. Also ist $y$ eine Lösung genau dann, wenn die Funktion $x\mapsto y(x)e^{-x}$ konstant ist,
d.h. $y(x)e^{-x}=c$ für alle $x$, oder $y(x)=ce^x$.
\end{enumerate}


\end{bsp}


 Für eine DGl kann man mehrere Problemstellungen betrachten:
\begin{itemize}
\item Finde alle Lösungen (man sagt oft ``finde die allgemeine Lösung'').  In den obigen Beispielen haben wir schon gesehen, dass eine Differentialgleichung
unendlich viele Lösungen haben kann. Allerdings wissen wir noch nicht, ob alle DGl überhaupt Lösungen haben.\footnote{Die berühmten Navier-Stokes-Gleichungen sind sehr wichtige (partielle) Differentialgleichungen der Strömungsmechanik, die seit langem in der Modellierung verwendet werden. Allerdings gibt es immer noch keinen strikten mathematischen Beweis für die Existenz und Eindeutigkeit von glatten Lösungen. Auch dieses Problem
gehört zu den sieben Millenium-Problemen der Mathematik (mit einem Preisgeld von einer Million US-Dollar).}

\item Finde eine Lösung, die eine zusätzliche Bedingung erfüllt. Wir betrachten vor allem die Anfangsbedingung $y(x_0)=y_0$, wobei $x_0$ und $y_0$
vorgegebene Zahlen sind. Diese Formulierung nennt man \emph{Anfangswertproblem} (AWP). Natürlich muss man verstehen ob es eine solche Lösung existiert und, falls ja, ob so eine Lösung eindeutig bestimmt ist.
\end{itemize}

Um diese (und auch andere) Problemstellungen für allgemeine Differentialgleichungen zu behandeln, werden wir in kommenden Wochen neue abstrakte Begriffe einführen und eine ziemlich komplexe mathematische Theorie entwickeln müssen.


\begin{bsp}
\begin{enumerate}
\item Sei $f:I\to \RR$ eine stetige Funktion. Betrachte die Differentialgleichung $y'(x)=f(x)$. Wie schon oben gemerkt, bilden die Stammfunktionen von $f$
die Menge der Lösungen. Sei $x_0\in I$ fest, dann hat die allgemeine Lösung die Form
\[
y(x)=C+\int_{x_0}^x f,
\]
wobei $C$ eine beliebige Konstante ist. Wir suchen jetzt nach einer Lösung $y$, die die Anfangsbedingung $y(x_0)=y_0$ erfüllt. Es ist einfach zu prüfen, dass die
einzige Lösung mit dieser Eigenschaft durch
$y(x)=y_0+\dint_{x_0}^x f$ gegeben ist. Z.B. ist $y(x)=(x-1)^2$ die einzige Lösung der Gleichung $y'(x)=2(x-1)$ mit der Anfangsbedingung $y(1)=0$.
Wichtig ist, dass jede Lösung auf dem ganzen Intervall $I$ definiert ist.

\item Betrachte jetzt die Differentialgleichung $y'(x)=y(x)$. Wir haben oben gesehen, dass die allgemeine Lösung $y(x)=ce^{x}$ ist.
Wir suchen jetzt nach einer Lösung, die die Anfangsbedingung $y(x_0)=y_0$ erfüllt: man muss also $ce^{x_0}=y_0$ haben, d.h. $c=y_0e^{-x_0}$,
und die entsprechende (eindeutig bestimmte) Lösung ist $y(x)=y_0e^{x-x_0}$. Z.B. ist $y(x)=-3e^{x-1}$ die einzige Lösung mit $y(1)=-3$.
Die Lösungen sind auf dem ganzen $\RR$ definiert.
\end{enumerate}
\end{bsp}


\subsection*{Lineare Differentialgleichung erster Ordnung}

Wir werden zuerst eine Klasse von Differentialgleichungen betrachten, für die es eine ziemlich direkte Lösungsmethode existiert, und die
die oben betrachtete Beispiele einschliesst.

Eine \emph{lineare Differentialgleichung erster Ordnung} ist eine Differentialgleichung
der Form
\begin{equation}
  \label{ldgl}
y'(x)=a(x)y(x)+b(x),
\end{equation}
wobei $I\subset\RR$ ein Intervall und $a,b:I\to\RR$ stetige Funktionen sind.

\begin{bsp}[Populationsdynamik]
Die DGl \eqref{ldgl} wird oft in der Modellierung\footnote{Mehr dazu erfährt man in der Lehrveranstaltung ``mat320 - Mathematische Modellierung''} von verschiedenen Wachstumsprozessen verwendet. Zum Beispiel, sei $y(t)$ die Grösse einer Population zum Zeitpunkt $t$
und seien $t_j$ und $t_{j+1}$ zwei Zeipunkte. Wir nehmen an, dass
\[
y(t_{j+1})-y(t_j)=\Delta G(t_j) -\Delta S(t_j)+\Delta M(t_j)
\]
gilt, wobei
\begin{itemize}
\item $\Delta G(t_j)=$ Anzahl der Geburten im Zeitintervall $[t_j,t_{j+1}]$,
\item $\Delta S(t_j)=$ Anzahl der Sterbefälle im Zeitintervall $[t_j,t_{j+1}]$,
\item $\Delta M(t_j)=$ Migration (Ab- und Zuwanderung) im Zeitintervall $[t_j,t_{j+1}]$, kann $\ge 0$ oder $\le 0$ sein.
\end{itemize}
Bezeichnet man $\Delta t:=t_{j+1}-t_j$, so erhält man die Relation
\[
\dfrac{y(t_{j+1})-y(t_j)}{\Delta t}=\dfrac{\Delta G(t_j)}{\Delta t} -\dfrac{\Delta S(t_j)}{\Delta t}+\dfrac{\Delta M(t_j)}{\Delta t}.
\]
Für $\Delta t\to 0$ erhält man also $y'(t)=g(t)-s(t)+m(t)$,
wobei
\[
g(t)=\lim_{\Delta t\to 0}\dfrac{\Delta G}{\Delta t},
\quad
s(t)=\lim_{\Delta t\to 0}\dfrac{\Delta S}{\Delta t},
\quad
m(t)=\lim_{\Delta t\to 0}\dfrac{\Delta M}{\Delta t}.
\]
Oft betrachtet man folgende zusätzliche Hypothesen:
\begin{itemize}
\item $g(t)$ (``Anzahl der Geburte pro Zeiteinheit'') ist linear proportional zum aktuellen Bestand der Population, $g(t)=\alpha(t)y(t)$,
\item $s(t)$ (``Anzahl der Sterbefälle pro Zeiteinheit'') ist linear proportional zum aktuellen Bestand der Population, $s(t)=\beta(t)y(t)$,
\item $m(t)$ (``Anzahl der Migranten pro Zeiteinheit'') ist unabhängig vom aktuellen Bestand der Population.
\end{itemize}
Dadurch erhält man die DGl $y'(t)=a(t)y(t)+m(t)$ mit $a(t)=\alpha(t)-\beta(t)$.
Die Funktionen $\alpha$, $\beta$ und $m$ bestimmt man typischeweise durch Analyse von statistischen Daten. \qed
\end{bsp}

 Falls $b\not\equiv 0$, wird die DGl \eqref{ldgl} \emph{inhomogen} genannt, dann betrachtet man
zusätzlich die zugehörige \emph{homogene} lineare Differentialgleichung
\begin{equation}
  \label{ldgl0}
y'(x)=a(x)y(x),
\end{equation}
(d.h. ohne den Term $b$). Die Gleichungen sind eng miteinander verbunden:
\begin{prop}
Sei $z$ eine Lösung der inhomogenen DGl \eqref{ldgl} auf einem Intervall $J\subset I$. Eine Funktion $y$ ist Lösung derselben inhomogenen DGl \eqref{ldgl} auf $J$ genau dann, wenn die Differenz $y_0:=y-z$ Lösung der homogenen Gleichung \eqref{ldgl0} auf $J$ ist. 
\end{prop}

\begin{proof}
Wir haben $z'=az+b$ auf $J$.

Sei $y_0$ Lösung der homogenen DGl \eqref{ldgl0}, d.h. $y_0'=a y_0$, dann gilt für $y:=y_0+z$:
\[
y'=y_0'+z'=ay_0+az+b=a(y_0+z)+b=ay+b,
\]
also ist $y$ Lösung der inhomogenen DGl \eqref{ldgl}. Umgekehrt, sei $y$ Lösung der inhomogenen DGl \eqref{ldgl}, d.h. $y'=ay+b$, dann
gilt für $y_0:=y-z$:
\[
y_0'=y'-z'= ay+b -(az+b)=a(y-z)=ay_0,
\]
also ist $y_0$ Lösung der homogenen DGl \eqref{ldgl0}.
\end{proof}

Diese Rechnungen kann man wie folgt zusammenfassen:

\begin{korol}
Die allgemeine Lösung der inhomogenen DGl = die allgemeine Lösung der homogenen DGl + eine beliebige (``spezielle'') Lösung der inhomogenen DGl.
\end{korol}


Das Problem \eqref{ldgl} löst man also in zwei Schritten:
\begin{itemize}
\item Man findet die allgemeine Lösung $y_h$ der homogenen Gleichung,
\item Man findet irgendeine (``spezielle'') Lösung $y_s$ der inhomogenen Gleichung.
\end{itemize}
Diese Schritte werden wir jetzt einzeln betrachten:

\begin{satz}[Allgemeine Lösung der homogenen Gleichung]
Sei $A$ eine Stammfunktion von $a$ auf $I$. Die allgemeine Lösung $y_h$ der homogenen Differentialgleichung
\eqref{ldgl0} hat die Form $y_h(x)=C e^{A(x)}$, wobei $C$ beliebige Konstante ist.
\end{satz}

\begin{proof}
Da $e^{-A(x)}$ nie gleich Null ist, kann man die beiden Seiten von \eqref{ldgl0} mit $e^{-A(x)}$ multiplizieren und dann alle Terme von der rechten auf die linke Seite
bringen um eine äquivalente Differentialgleichung zu erhalten,
\[
e^{-A(x)}y'(x) -a(x) y =0.
\]
Wegen $A'=a$ kann man diese Gleichung einfach als $(e^{-A} y)'=0$ umschreiben. Eine Funktion $y$ ist Lösung von \eqref{ldgl0} genau dann, wenn $e^{-A} y$ konstant ist,
$e^{-A} y=C$, und dann $y=C e^A$.
\end{proof}

In der Sprache der linearen Algebra, bilden die Lösungen der homogenen Gleichung \eqref{ldgl0} einen (eindimensionalen) Vektorraum,
und die Lösungsmenge zu \eqref{ldgl} ist ein affiner Raum. Diese Sprache werden wir aber erst später nutzen (bei der Untersuchung von Systemen linearer Differentialgleichungen)


Um \emph{alle} Lösungen der inhomogenen Gleichung \eqref{ldgl} zu finden, brauchen wir jetzt nur \emph{eine} Lösung davon. Diese wird oft durch die sogenannte \emph{Variation der Konstanten} gefunden.
Nämlich, suchen wir nach einer Lösung der Form $y(x)=C(x)e^{A(x)}$, d.h. man nehme die allgemeine Lösung der homogenen Gleichung und ersetze die Konstante $C$ durch
eine \emph{Funktion} $C(x)$. Diese Funktion $y$ setzt man in die DGl \eqref{ldgl} ein:
\[
( C e^A)'=a Ce^A+b, \text{ oder } C'e^{A}+CA' E^A=Ca e^A+b, \text{ oder } C'e^{A}=b.
\]
(Im letzen Schritt haben wir $A'=a$ wieder verwendet.) Es folgt, dass $y=Ce^A$ genau dann Lösung der inhomogenen Gleichung \eqref{ldgl} ist, wenn die Funktion
$C$ die Gleichung $C'=be^{-A}$ erfüllt. Diese neue Gleichung für $C$ lässt sich lösen: die Lösungen sind genau die Stammfunktionen von $be^{-A}$, also
\begin{equation}
  \label{eqc0}
C(x)=\int b(x)e^{-A(x)}dx.
\end{equation}
\begin{satz}[Allgemeine Lösung einer linearen Differentialgleichung erster Ordnung]
Die allgemeine Lösung $y$ der inhomogenen Gleichung \eqref{ldgl} hat die Form
\[
y=\Big( C+ \dint b e^{-A}\Big) e^A, \quad C\in\RR,
\]
wobei $A$ eine beliebige Stammfunktion von $a$ ist.
\end{satz}
Man merkt auch, dass alle Lösungen auf dem ganzen Definitionsbereich $I$ der Koeffizienten $a$ und $b$ definiert sind:
das ist eine besondere Eigenschaft von linearen Differentialgleichungen.

Jetzt werden wir die Anfangsbedingung berücksichtigen:
\begin{satz}
Seien $x_0\in I$ und $y_0\in\RR$. Dann besitzt die lineare inhomogene Gleichung \eqref{ldgl} genau eine Lösung $y$ zum AWP $y(x_0)=y_0$. Diese Lösung ist durch
\[
y(x)=y_0\exp\Big( \int_{x_0}^x a(s)\, ds\Big) + \int_{x_0}^x b(t) \exp\Big( \int_t^x a(s)\, ds\Big)dt
\]
gegeben.
\end{satz}
\begin{proof}
Wir müssen die vorherigen Konstruktionen wiederholen, aber jetzt werden alle Konstanten wichtig. Als Stammfunktion von $a$ wählen wir
$A(x)=\dint_{x_0}^x a(t)\, dt$.
Auch für \eqref{eqc0} wählen wir eine ganz bestimmte Lösung,
\[
C(x)=\int_{x_0}^x b(t) e^{-A(t)}dt\equiv \int_{x_0}^x b(t) \exp\Big( \int_t^{x_0} a(s)\, ds\Big)dt,
\]
dadurch erhalten wir den folgenden Ausdruck für die allgemeine Lösung:
\begin{align*}
y(x)&=c \exp\Big( \int_{x_0}^x a(s)\, ds\Big) + \int_{x_0}^x b(t) \exp\Big( \int_t^{x_0} a(s)\, ds\Big)dt\, \exp\Big( \int_{x_0}^x a(s)\, ds\Big)\\
&=c \exp\Big( \int_{x_0}^x a(s)\, ds\Big) + \int_{x_0}^x b(t) \exp\Big( \int_t^x a(s)\, ds\Big)dt, \quad c\in\RR.
\end{align*}
Um die Bedingung $y(x_0)=y_0$ zu erfüllen muss man lediglich $c=y_0$ nehmen.
\end{proof}


\begin{bsp}
Differentialgleichung $y'(x)= 2x y(x)+x$ und AWP $y(0)=0$.


Das ist eine lineare Differentialgleichung \eqref{ldgl} mit $a(x)=2x$ und $b(x)=x$. Als Stammfunktion von $a$ nehmen wir $A(x)=x^2$.
Die allgemeine Lösung der homogenen Gleichung ist also $y_h(x)=ce^{x^2}$. Darüber hinaus haben wir eine spezielle Lösung $y_s(x)=C(x)e^{x^2}$ mit
$C=\dint x e^{-x^2}\, dx$.
Mit Hilfe der Substitution $\varphi(x)=x^2$ findet man $C(x)=-\frac{1}{2}\,e^{-x^2}$, und die entsprechende spezielle Lösung ist $y_s(x)=-\frac{1}{2}e^{-x^2}e^{x^2}=-\dfrac{1}{2}$. Die allgemeine Lösung ist $y(x)=y_h(x)+y_s(x)=ce^{x^2}-\frac{1}{2}$.

Diese Funktion $y$ erfüllt die Bedingung $y(0)=0$ für $c=\frac 12$, also ist $y(x)=\frac{1}{2}\,e^{x^2}-\frac{1}{2}$ die gesuchte Lösung des AWPs.
\end{bsp}

\subsection*{Separation der Variablen}

Wir betrachten jetzt die Differentialgleichungen der Form
\begin{equation}
  \label{separ0}
y'(x)=\dfrac{f(x)}{g\big(y(x)\big)},
\end{equation}
wobei $g\ne 0$. Das ist eine weitere wichtige Klasse von Differentialgleichungen, deren Lösungen man mit Hilfe von Integralen schreiben kann.

Diese kann man zuerst als $g\big(y(x)\big) y'(x)=f(x)$ umschreiben.
Ist $G$ eine Stammfunktion von $g$ und $F$ eine Stammfunktion von $f$, so gilt $(G\circ y)'=F'$, also
$G\big(y(x)\big)=F(x)+C$ für eine Konstante $C\in\RR$. Das wird meist in vereinfachter symmetrischer Form geschrieben,
\[
\dint f(y)\, dy=\dint g(x)\, dx.
\]
Kann man die Funktion $G$ umkehren, so ist $y(x)=G^{-1}\big(F(x)+C\big)$ Lösung von \eqref{separ0}.

Wir werden die obige Rechnung als Satz formulieren (die Existenz der Stammfunktionen wird durch die Stetigkeit der Funktionen $f$ und $g$ garantiert):
\begin{satz}
Seien $I,J$ Intervalle, $f:I\to\RR$ und $g:J\to\RR$ stetige Funktionen mit $g(y)\ne 0$ für alle $y\in J$. Seien $F$ ung $G$ Stammfunktionen von $f$ und $g$.
Eine differenzierbare Funktion $y:I'\to J$, die auf einem Teilintervall $I'\subset I$ definiert ist, ist genau dann Lösung der Differentialgleichung \eqref{separ0}, wenn
sie die Gleichung $G\big(y(x)\big)=F(x)+C$ mit einer Konstante $C$ erfüllt.
\end{satz}


Die Anfangsbedingung  $y(x_0)=y_0$ kann man auch berücksichtigen: sie ist zu $G(y_0)=F(x_0)+C$ äquivalent, d.h.
$C=G(x_0)-F(x_0)$. Also ist eine Funktion $y$ Lösung der Differentialgleichung mit $y(x_0)=y_0$ genau dann, wenn sie
\[
G\big(y(x)\big)-G(y_0)=F(x)-F(x_0).
\]
für alle $x$ erfüllt. Nutzt man den Hauptsatz der Differential- und Integralrechnung, so erhält man die folgende Gleichung
für $y$:
\[
\int_{y_0}^{y(x)} g = \int_{x_0}^x f.
\]
Im Einzelfall muss dann noch untersucht werden, auf welchem Intervall diese Lösung $y$ definiert ist.


\begin{bsp}
Differentialgleichung $y'(x)=y(x)^2$.


Ganz formell sind wir schon im Rahmen der Separation der Variablen: $y(x)^2=f(x)/g\big(y(x)\big)$ mit $f(x)=1$ und $g(y)=1/y^2$.
\[
\dfrac{y'(x)}{y(x)^2}=1, \quad \dint \dfrac{dy}{y^2}=\dint dx, \quad -\dfrac{1}{y}=x+C, \quad y(x)=-\dfrac{1}{x+C}.
\]
Diese Konstruktion gilt aber nur im Bereich, in dem die Funktion $g$ definiert ist und nicht verschwindet. In unserem Fall
ist $g(y)$ für $y\ne 0$ definiert, für alle solchen $y$ gilt $g(y)\ne 0$. Man  muss also zusätzlich prüfen, ob es Lösungen
$y$ existieren, die verschwinden. Wir sehen also folgendes:
\begin{itemize}
\item Die konstante Funktion $y(x)\equiv 0$ ist bestimmt Lösung.

\item Alle anderen Lösungen können nicht verschwinden: gilt $y(x)\ne 0$ für ein $x$, so ist $y$ automatisch durch $y(x)=-1/(x+C)$ mit einer Konstante $C$ gegeben.

\end{itemize}

Wir suchen jetzt nach den Lösungen, die die Anfangsbedingung $y(x_0)=y_0$ erfüllen.
\begin{itemize}
\item Für $y_0=0$ ist $y(x)=0$ die einzige Lösung mit dieser Eigenschaft. Diese Lösung ist auf dem ganzen $\RR$ definiert.
\item Für $y_0\ne 0$ muss die gesuchte Lösung $y$ die Form $y(x)=-1/(x+C)$ haben. Für $x=x_0$ erhält man $y_0=-1/(x_0+C)$, dadurch findet man den Wert der Konstante $C$:
$C=-1/y_0 - x_0$, also ist $y(x)=\dfrac{y_0}{1-y_0(x-x_0)}$ die Lösung des AWPs. Diese Lösungen sind aber nicht auf dem ganzen $\RR$ definiert: der Punkt $x=x_0+1/y_0$
ist nicht im Definitionsbereich, und wegen $\lim_{x\to x_0+1/y_0} y(x)=\infty$ ist keine stetige Fortsetung in diesen Punkt möglich.
\end{itemize}

Wir sehen hier eine wichtige  Eigenschaft: der Definitionsbereich der Lösung ist im Allgemeinen von der Anfangsbedingung abhängig. Allerdings haben wir zu jeder
Anfangsbedingung nur eine einzige Lösung.
\end{bsp}


\begin{bsp} Die Differentialgleichung $y'(x)=2\sqrt{y(x)}$ ist nur für $y(x)\ge 0$ definiert. Wir haben wieder die konstante Lösung $y(x)=0$. Die Lösungen mit $y(x)> 0$ findet man mit der Separation der Variablen:
\[
\dint \dfrac{dy}{2\sqrt{y}}=\dint dx, \quad \sqrt{y}=x+C, \quad y(x)=(x+C)^2,
\]
wobei $x+C> 0$ erfüllt sein muss. Diese Funktionen können aber verschwinden: $\lim_{x\to -C}y(x)=0$, und man kann sie mit der konstanten Lösung $y(x)=0$ ``zusammenkleben''
um eine auf dem ganzen $\RR$ definierte Lösung zu erhalten.
Es folgt daraus, dass
das AWP $y(0)=0$ mehrere Lösungen hat, z.B. $y(x)\equiv 0$, oder $y(x)=x^2$, oder $y(x)=\begin{cases}
0, & x\le c,\\
(x-c)^2, & x>c
\end{cases}$
für beliebige $c>0$. Also kann ein AWP mehrere Lösungen haben.
\end{bsp}


\end{document}
